{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.glorot_normal()\n",
    "\n",
    "\n",
    "encoder_input = tf.keras.Input(shape=(1))\n",
    "x = layers.Dense(24, activation=\"relu\", kernel_initializer=initializer)(encoder_input)\n",
    "x = layers.Dense(24, activation=\"relu\",kernel_initializer=initializer)(x)\n",
    "#x = layers.Dense(24, activation=\"softplus\",kernel_initializer=initializer)(x)\n",
    "#x = layers.Dense(24, activation=\"softplus\",kernel_initializer=initializer)(x)\n",
    "#x = layers.Dense(16, activation=\"tanh\",kernel_initializer=initializer)(x)\n",
    "encoder_output = layers.Dense(1)(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 24)                48        \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "Training loss in direct  step: 0 0.000585808593314141\n",
      "Training loss in direct  step: 20 0.0005757245235145092\n",
      "Training loss in direct  step: 40 0.0005662681651301682\n",
      "Training loss in direct  step: 60 0.0005573764210566878\n",
      "Training loss in direct  step: 80 0.000549007672816515\n",
      "Training loss in direct  step: 100 0.0005411249585449696\n",
      "Training loss in direct  step: 120 0.0005336840986274183\n",
      "Training loss in direct  step: 140 0.0005266641965135932\n",
      "Training loss in direct  step: 160 0.0005200371379032731\n",
      "Training loss in direct  step: 180 0.0005138098495081067\n",
      "Training loss in direct  step: 200 0.0005079369875602424\n",
      "Training loss in direct  step: 220 0.0005023924168199301\n",
      "Training loss in direct  step: 240 0.0004971402231603861\n",
      "Training loss in direct  step: 260 0.0004921674262732267\n",
      "Training loss in direct  step: 280 0.0004874651203863323\n",
      "Training loss in direct  step: 300 0.00048301712376996875\n",
      "Training loss in direct  step: 320 0.00047879695193842053\n",
      "Training loss in direct  step: 340 0.00047479436034336686\n",
      "Training loss in direct  step: 360 0.0004710058856289834\n",
      "Training loss in direct  step: 380 0.00046741418191231787\n",
      "Training loss in direct  step: 400 0.0004640048136934638\n",
      "Training loss in direct  step: 420 0.0004607688752003014\n",
      "Training loss in direct  step: 440 0.0004576902720145881\n",
      "Training loss in direct  step: 460 0.00045476562809199095\n",
      "Training loss in direct  step: 480 0.00045198059524409473\n",
      "Training loss in direct  step: 500 0.0004493331944104284\n",
      "Training loss in direct  step: 520 0.00044681940926238894\n",
      "Training loss in direct  step: 540 0.00044443088700063527\n",
      "Training loss in direct  step: 560 0.0004421681514941156\n",
      "Training loss in direct  step: 580 0.00044001854257658124\n",
      "Training loss in direct  step: 600 0.0004379727761261165\n",
      "Training loss in direct  step: 620 0.0004360325401648879\n",
      "Training loss in direct  step: 640 0.00043418919085524976\n",
      "Training loss in direct  step: 660 0.0004324350447859615\n",
      "Training loss in direct  step: 680 0.0004307664930820465\n",
      "Training loss in direct  step: 700 0.00042918362305499613\n",
      "Training loss in direct  step: 720 0.00042767400736920536\n",
      "Training loss in direct  step: 740 0.0004262380243744701\n",
      "Training loss in direct  step: 760 0.000424869213020429\n",
      "Training loss in direct  step: 780 0.0004235672822687775\n",
      "Training loss in direct  step: 800 0.000422328565036878\n",
      "Training loss in direct  step: 820 0.0004211544874124229\n",
      "Training loss in direct  step: 840 0.0004200375115033239\n",
      "Training loss in direct  step: 860 0.0004189707397017628\n",
      "Training loss in direct  step: 880 0.0004179584211669862\n",
      "Training loss in direct  step: 900 0.0004169975291006267\n",
      "Training loss in direct  step: 920 0.00041608454193919897\n",
      "Training loss in direct  step: 940 0.00041521398816257715\n",
      "Training loss in direct  step: 960 0.00041438109474256635\n",
      "Training loss in direct  step: 980 0.00041358888847753406\n",
      "Training loss in direct  step: 1000 0.0004128348082304001\n",
      "Training loss in direct  step: 1020 0.0004121160018257797\n",
      "Training loss in direct  step: 1040 0.00041142795816995203\n",
      "Training loss in direct  step: 1060 0.00041077929199673235\n",
      "Training loss in direct  step: 1080 0.0004101620870642364\n",
      "Training loss in direct  step: 1100 0.0004095746553502977\n",
      "Training loss in direct  step: 1120 0.0004090122238267213\n",
      "Training loss in direct  step: 1140 0.00040847621858119965\n",
      "Training loss in direct  step: 1160 0.00040796599932946265\n",
      "Training loss in direct  step: 1180 0.0004074840690009296\n",
      "Training loss in direct  step: 1200 0.0004070249560754746\n",
      "Training loss in direct  step: 1220 0.0004065898829139769\n",
      "Training loss in direct  step: 1240 0.00040617675404064357\n",
      "Training loss in direct  step: 1260 0.00040577907930128276\n",
      "Training loss in direct  step: 1280 0.00040540003101341426\n",
      "Training loss in direct  step: 1300 0.0004050381830893457\n",
      "Training loss in direct  step: 1320 0.0004046912072226405\n",
      "Training loss in direct  step: 1340 0.00040435834671370685\n",
      "Training loss in direct  step: 1360 0.00040404312312602997\n",
      "Training loss in direct  step: 1380 0.000403743761125952\n",
      "Training loss in direct  step: 1400 0.0004034580197185278\n",
      "Training loss in direct  step: 1420 0.00040318493847735226\n",
      "Training loss in direct  step: 1440 0.0004029233823530376\n",
      "Training loss in direct  step: 1460 0.00040267518488690257\n",
      "Training loss in direct  step: 1480 0.00040243699913844466\n",
      "Training loss in direct  step: 1500 0.0004022098728455603\n",
      "Training loss in direct  step: 1520 0.0004019913903903216\n",
      "Training loss in direct  step: 1540 0.00040178102790378034\n",
      "Training loss in direct  step: 1560 0.00040157948387786746\n",
      "Training loss in direct  step: 1580 0.00040138649637810886\n",
      "Training loss in direct  step: 1600 0.00040120110497809947\n",
      "Training loss in direct  step: 1620 0.0004010229604318738\n",
      "Training loss in direct  step: 1640 0.0004008576215710491\n",
      "Training loss in direct  step: 1660 0.0004006984818261117\n",
      "Training loss in direct  step: 1680 0.00040054856799542904\n",
      "Training loss in direct  step: 1700 0.0004004047950729728\n",
      "Training loss in direct  step: 1720 0.00040027074282988906\n",
      "Training loss in direct  step: 1740 0.00040014192927628756\n",
      "Training loss in direct  step: 1760 0.000400020566303283\n",
      "Training loss in direct  step: 1780 0.0003999049949925393\n",
      "Training loss in direct  step: 1800 0.0003997926542069763\n",
      "Training loss in direct  step: 1820 0.0003996858140453696\n",
      "Training loss in direct  step: 1840 0.0003995818260591477\n",
      "Training loss in direct  step: 1860 0.0003994824073743075\n",
      "Training loss in direct  step: 1880 0.00039938741247169673\n",
      "Training loss in direct  step: 1900 0.0003992946003563702\n",
      "Training loss in direct  step: 1920 0.0003992060665041208\n",
      "Training loss in direct  step: 1940 0.00039912015199661255\n",
      "Training loss in direct  step: 1960 0.00039903781726025045\n",
      "Training loss in direct  step: 1980 0.00039895877125672996\n",
      "Training loss in direct  step: 2000 0.0003988824028056115\n",
      "Training loss in direct  step: 2020 0.00039881019620224833\n",
      "Training loss in direct  step: 2040 0.00039874151116237044\n",
      "Training loss in direct  step: 2060 0.00039867631858214736\n",
      "Training loss in direct  step: 2080 0.0003986138035543263\n",
      "Training loss in direct  step: 2100 0.0003985530638601631\n",
      "Training loss in direct  step: 2120 0.0003984944196417928\n",
      "Training loss in direct  step: 2140 0.00039843766717240214\n",
      "Training loss in direct  step: 2160 0.00039838318480178714\n",
      "Training loss in direct  step: 2180 0.00039833105984143913\n",
      "Training loss in direct  step: 2200 0.00039828187436796725\n",
      "Training loss in direct  step: 2220 0.00039823155384510756\n",
      "Training loss in direct  step: 2240 0.00039818481309339404\n",
      "Training loss in direct  step: 2260 0.0003981383051723242\n",
      "Training loss in direct  step: 2280 0.00039809499867260456\n",
      "Training loss in direct  step: 2300 0.00039805396227166057\n",
      "Training loss in direct  step: 2320 0.00039801374077796936\n",
      "Training loss in direct  step: 2340 0.0003979768662247807\n",
      "Training loss in direct  step: 2360 0.0003979405446443707\n",
      "Training loss in direct  step: 2380 0.00039790512528270483\n",
      "Training loss in direct  step: 2400 0.00039787113200873137\n",
      "Training loss in direct  step: 2420 0.00039783812826499343\n",
      "Training loss in direct  step: 2440 0.0003978061431553215\n",
      "Training loss in direct  step: 2460 0.00039777575875632465\n",
      "Training loss in direct  step: 2480 0.00039774778997525573\n",
      "Training loss in direct  step: 2500 0.0003977202286478132\n",
      "Training loss in direct  step: 2520 0.0003976941807195544\n",
      "Training loss in direct  step: 2540 0.00039766839472576976\n",
      "Training loss in direct  step: 2560 0.00039764397661201656\n",
      "Training loss in direct  step: 2580 0.0003976198786403984\n",
      "Training loss in direct  step: 2600 0.0003975975268986076\n",
      "Training loss in direct  step: 2620 0.00039757491322234273\n",
      "Training loss in direct  step: 2640 0.00039755343459546566\n",
      "Training loss in direct  step: 2660 0.0003975328872911632\n",
      "Training loss in direct  step: 2680 0.00039751254371367395\n",
      "Training loss in direct  step: 2700 0.00039749257848598063\n",
      "Training loss in direct  step: 2720 0.0003974735445808619\n",
      "Training loss in direct  step: 2740 0.0003974554711021483\n",
      "Training loss in direct  step: 2760 0.0003974383289460093\n",
      "Training loss in direct  step: 2780 0.0003974217106588185\n",
      "Training loss in direct  step: 2800 0.0003974056744482368\n",
      "Training loss in direct  step: 2820 0.00039738978375680745\n",
      "Training loss in direct  step: 2840 0.0003973740094806999\n",
      "Training loss in direct  step: 2860 0.0003973594284616411\n",
      "Training loss in direct  step: 2880 0.00039734531310386956\n",
      "Training loss in direct  step: 2900 0.0003973316343035549\n",
      "Training loss in direct  step: 2920 0.00039731807191856205\n",
      "Training loss in direct  step: 2940 0.0003973045095335692\n",
      "Training loss in direct  step: 2960 0.0003972922277171165\n",
      "Training loss in direct  step: 2980 0.00039728000410832465\n",
      "Training loss in direct  step: 3000 0.00039726748946122825\n",
      "Training loss in direct  step: 3020 0.0003972554695792496\n",
      "Training loss in direct  step: 3040 0.00039724388625472784\n",
      "Training loss in direct  step: 3060 0.00039723224472254515\n",
      "Training loss in direct  step: 3080 0.00039722229121252894\n",
      "Training loss in direct  step: 3100 0.00039721105713397264\n",
      "Training loss in direct  step: 3120 0.0003972007834818214\n",
      "Training loss in direct  step: 3140 0.0003971897531300783\n",
      "Training loss in direct  step: 3160 0.00039717936306260526\n",
      "Training loss in direct  step: 3180 0.0003971693222410977\n",
      "Training loss in direct  step: 3200 0.00039715945604257286\n",
      "Training loss in direct  step: 3220 0.00039714970625936985\n",
      "Training loss in direct  step: 3240 0.00039714027661830187\n",
      "Training loss in direct  step: 3260 0.00039713081787340343\n",
      "Training loss in direct  step: 3280 0.00039712167927064\n",
      "Training loss in direct  step: 3300 0.0003971122205257416\n",
      "Training loss in direct  step: 3320 0.0003971029072999954\n",
      "Training loss in direct  step: 3340 0.00039709408883936703\n",
      "Training loss in direct  step: 3360 0.00039708518306724727\n",
      "Training loss in direct  step: 3380 0.00039707697578705847\n",
      "Training loss in direct  step: 3400 0.0003970680700149387\n",
      "Training loss in direct  step: 3420 0.00039705997915007174\n",
      "Training loss in direct  step: 3440 0.00039705209201201797\n",
      "Training loss in direct  step: 3460 0.00039704423397779465\n",
      "Training loss in direct  step: 3480 0.0003970357065554708\n",
      "Training loss in direct  step: 3500 0.0003970276447944343\n",
      "Training loss in direct  step: 3520 0.00039701946661807597\n",
      "Training loss in direct  step: 3540 0.00039701155037619174\n",
      "Training loss in direct  step: 3560 0.00039700334309600294\n",
      "Training loss in direct  step: 3580 0.0003969958343077451\n",
      "Training loss in direct  step: 3600 0.000396988180000335\n",
      "Training loss in direct  step: 3620 0.00039698078762739897\n",
      "Training loss in direct  step: 3640 0.00039697240572422743\n",
      "Training loss in direct  step: 3660 0.0003969649551436305\n",
      "Training loss in direct  step: 3680 0.0003969577082898468\n",
      "Training loss in direct  step: 3700 0.00039695025770924985\n",
      "Training loss in direct  step: 3720 0.00039694368024356663\n",
      "Training loss in direct  step: 3740 0.00039693620055913925\n",
      "Training loss in direct  step: 3760 0.0003969295939896256\n",
      "Training loss in direct  step: 3780 0.00039692281279712915\n",
      "Training loss in direct  step: 3800 0.000396916875615716\n",
      "Training loss in direct  step: 3820 0.00039690962876193225\n",
      "Training loss in direct  step: 3840 0.0003969036915805191\n",
      "Training loss in direct  step: 3860 0.00039689778350293636\n",
      "Training loss in direct  step: 3880 0.00039689161349087954\n",
      "Training loss in direct  step: 3900 0.00039688555989414454\n",
      "Training loss in direct  step: 3920 0.000396880874177441\n",
      "Training loss in direct  step: 3940 0.00039687511161901057\n",
      "Training loss in direct  step: 3960 0.0003968701639678329\n",
      "Training loss in direct  step: 3980 0.00039686457603238523\n",
      "Training loss in direct  step: 4000 0.00039685898809693754\n",
      "Training loss in direct  step: 4020 0.00039685360388830304\n",
      "Training loss in direct  step: 4040 0.00039684853982180357\n",
      "Training loss in direct  step: 4060 0.00039684289367869496\n",
      "Training loss in direct  step: 4080 0.00039683812065050006\n",
      "Training loss in direct  step: 4100 0.0003968329692725092\n",
      "Training loss in direct  step: 4120 0.0003968276141677052\n",
      "Training loss in direct  step: 4140 0.0003968222299590707\n",
      "Training loss in direct  step: 4160 0.0003968165838159621\n",
      "Training loss in direct  step: 4180 0.00039681143243797123\n",
      "Training loss in direct  step: 4200 0.00039680703775957227\n",
      "Training loss in direct  step: 4220 0.00039680220652371645\n",
      "Training loss in direct  step: 4240 0.00039679708424955606\n",
      "Training loss in direct  step: 4260 0.0003967922821175307\n",
      "Training loss in direct  step: 4280 0.00039678701432421803\n",
      "Training loss in direct  step: 4300 0.00039678133907727897\n",
      "Training loss in direct  step: 4320 0.00039677665336057544\n",
      "Training loss in direct  step: 4340 0.00039677173481322825\n",
      "Training loss in direct  step: 4360 0.00039676696178503335\n",
      "Training loss in direct  step: 4380 0.000396761461161077\n",
      "Training loss in direct  step: 4400 0.00039675753214396536\n",
      "Training loss in direct  step: 4420 0.00039675281732343137\n",
      "Training loss in direct  step: 4440 0.00039674798608757555\n",
      "Training loss in direct  step: 4460 0.00039674306754022837\n",
      "Training loss in direct  step: 4480 0.0003967388765886426\n",
      "Training loss in direct  step: 4500 0.00039673419087193906\n",
      "Training loss in direct  step: 4520 0.00039672947605140507\n",
      "Training loss in direct  step: 4540 0.00039672531420364976\n",
      "Training loss in direct  step: 4560 0.0003967206575907767\n",
      "Training loss in direct  step: 4580 0.0003967153315898031\n",
      "Training loss in direct  step: 4600 0.0003967097436543554\n",
      "Training loss in direct  step: 4620 0.0003967036318499595\n",
      "Training loss in direct  step: 4640 0.00039669813122600317\n",
      "Training loss in direct  step: 4660 0.00039669283432886004\n",
      "Training loss in direct  step: 4680 0.00039668739191256464\n",
      "Training loss in direct  step: 4700 0.0003966816875617951\n",
      "Training loss in direct  step: 4720 0.00039667595410719514\n",
      "Training loss in direct  step: 4740 0.0003966699878219515\n",
      "Training loss in direct  step: 4760 0.00039666451630182564\n",
      "Training loss in direct  step: 4780 0.0003966582880821079\n",
      "Training loss in direct  step: 4800 0.0003966530493926257\n",
      "Training loss in direct  step: 4820 0.00039664804353378713\n",
      "Training loss in direct  step: 4840 0.0003966432122979313\n",
      "Training loss in direct  step: 4860 0.0003966377698816359\n",
      "Training loss in direct  step: 4880 0.00039663302595727146\n",
      "Training loss in direct  step: 4900 0.00039662784547545016\n",
      "Training loss in direct  step: 4920 0.000396623247070238\n",
      "Training loss in direct  step: 4940 0.00039661809569224715\n",
      "Training loss in direct  step: 4960 0.00039661378832533956\n",
      "Training loss in direct  step: 4980 0.000396608782466501\n",
      "Training loss in direct  step: 5000 0.0003966052026953548\n",
      "Training loss in direct  step: 5020 0.00039660121547058225\n",
      "Training loss in direct  step: 5040 0.00039659684989601374\n",
      "Training loss in direct  step: 5060 0.00039659225149080157\n",
      "Training loss in direct  step: 5080 0.00039658910827711225\n",
      "Training loss in direct  step: 5100 0.0003965844225604087\n",
      "Training loss in direct  step: 5120 0.00039658084278926253\n",
      "Training loss in direct  step: 5140 0.00039657659363001585\n",
      "Training loss in direct  step: 5160 0.00039657301385886967\n",
      "Training loss in direct  step: 5180 0.00039656981243751943\n",
      "Training loss in direct  step: 5200 0.0003965664654970169\n",
      "Training loss in direct  step: 5220 0.0003965629730373621\n",
      "Training loss in direct  step: 5240 0.0003965593932662159\n",
      "Training loss in direct  step: 5260 0.00039655473665334284\n",
      "Training loss in direct  step: 5280 0.00039655304863117635\n",
      "Training loss in direct  step: 5300 0.0003965488576795906\n",
      "Training loss in direct  step: 5320 0.0003965452197007835\n",
      "Training loss in direct  step: 5340 0.00039654196007177234\n",
      "Training loss in direct  step: 5360 0.0003965387004427612\n",
      "Training loss in direct  step: 5380 0.00039653509156778455\n",
      "Training loss in direct  step: 5400 0.0003965310752391815\n",
      "Training loss in direct  step: 5420 0.0003965262440033257\n",
      "Training loss in direct  step: 5440 0.0003965217329096049\n",
      "Training loss in direct  step: 5460 0.0003965167561545968\n",
      "Training loss in direct  step: 5480 0.00039651221595704556\n",
      "Training loss in direct  step: 5500 0.0003965084324590862\n",
      "Training loss in direct  step: 5520 0.00039650389226153493\n",
      "Training loss in direct  step: 5540 0.0003964992647524923\n",
      "Training loss in direct  step: 5560 0.0003964954521507025\n",
      "Training loss in direct  step: 5580 0.00039649009704589844\n",
      "Training loss in direct  step: 5600 0.0003964866336900741\n",
      "Training loss in direct  step: 5620 0.00039648154051974416\n",
      "Training loss in direct  step: 5640 0.0003964772040490061\n",
      "Training loss in direct  step: 5660 0.0003964731877204031\n",
      "Training loss in direct  step: 5680 0.000396468851249665\n",
      "Training loss in direct  step: 5700 0.0003964641073253006\n",
      "Training loss in direct  step: 5720 0.00039645988726988435\n",
      "Training loss in direct  step: 5740 0.0003964547358918935\n",
      "Training loss in direct  step: 5760 0.000396451388951391\n",
      "Training loss in direct  step: 5780 0.00039644702337682247\n",
      "Training loss in direct  step: 5800 0.00039644393837079406\n",
      "Training loss in direct  step: 5820 0.0003964398056268692\n",
      "Training loss in direct  step: 5840 0.00039643573109060526\n",
      "Training loss in direct  step: 5860 0.00039643197669647634\n",
      "Training loss in direct  step: 5880 0.00039642828051000834\n",
      "Training loss in direct  step: 5900 0.0003964239440392703\n",
      "Training loss in direct  step: 5920 0.00039641931653022766\n",
      "Training loss in direct  step: 5940 0.00039641454350203276\n",
      "Training loss in direct  step: 5960 0.0003964104107581079\n",
      "Training loss in direct  step: 5980 0.0003964057832490653\n",
      "Training loss in direct  step: 6000 0.0003964017378166318\n",
      "Training loss in direct  step: 6020 0.0003963971103075892\n",
      "Training loss in direct  step: 6040 0.00039639323949813843\n",
      "Training loss in direct  step: 6060 0.0003963890776503831\n",
      "Training loss in direct  step: 6080 0.0003963849740102887\n",
      "Training loss in direct  step: 6100 0.0003963804920203984\n",
      "Training loss in direct  step: 6120 0.00039637580630369484\n",
      "Training loss in direct  step: 6140 0.00039637216832488775\n",
      "Training loss in direct  step: 6160 0.00039636678411625326\n",
      "Training loss in direct  step: 6180 0.00039636148721911013\n",
      "Training loss in direct  step: 6200 0.0003963562776334584\n",
      "Training loss in direct  step: 6220 0.0003963512135669589\n",
      "Training loss in direct  step: 6240 0.0003963462368119508\n",
      "Training loss in direct  step: 6260 0.00039634056156501174\n",
      "Training loss in direct  step: 6280 0.00039633564301766455\n",
      "Training loss in direct  step: 6300 0.0003963294147979468\n",
      "Training loss in direct  step: 6320 0.0003963244380429387\n",
      "Training loss in direct  step: 6340 0.00039631820982322097\n",
      "Training loss in direct  step: 6360 0.0003963125927839428\n",
      "Training loss in direct  step: 6380 0.0003963061317335814\n",
      "Training loss in direct  step: 6400 0.00039630060200579464\n",
      "Training loss in direct  step: 6420 0.0003962948976550251\n",
      "Training loss in direct  step: 6440 0.0003962890768889338\n",
      "Training loss in direct  step: 6460 0.00039628439117223024\n",
      "Training loss in direct  step: 6480 0.00039627886144444346\n",
      "Training loss in direct  step: 6500 0.0003962734481319785\n",
      "Training loss in direct  step: 6520 0.0003962671326007694\n",
      "Training loss in direct  step: 6540 0.0003962617483921349\n",
      "Training loss in direct  step: 6560 0.000396255956729874\n",
      "Training loss in direct  step: 6580 0.0003962496411986649\n",
      "Training loss in direct  step: 6600 0.0003962446062359959\n",
      "Training loss in direct  step: 6620 0.0003962390183005482\n",
      "Training loss in direct  step: 6640 0.00039623372140340507\n",
      "Training loss in direct  step: 6660 0.00039622728945687413\n",
      "Training loss in direct  step: 6680 0.00039622237090952694\n",
      "Training loss in direct  step: 6700 0.0003962172195315361\n",
      "Training loss in direct  step: 6720 0.0003962116315960884\n",
      "Training loss in direct  step: 6740 0.00039620668394491076\n",
      "Training loss in direct  step: 6760 0.0003962019691243768\n",
      "Training loss in direct  step: 6780 0.0003961966431234032\n",
      "Training loss in direct  step: 6800 0.0003961917827837169\n",
      "Training loss in direct  step: 6820 0.00039618636947125196\n",
      "Training loss in direct  step: 6840 0.0003961813054047525\n",
      "Training loss in direct  step: 6860 0.000396176939830184\n",
      "Training loss in direct  step: 6880 0.0003961721376981586\n",
      "Training loss in direct  step: 6900 0.0003961670445278287\n",
      "Training loss in direct  step: 6920 0.00039616250433027744\n",
      "Training loss in direct  step: 6940 0.0003961578768212348\n",
      "Training loss in direct  step: 6960 0.0003961539187002927\n",
      "Training loss in direct  step: 6980 0.0003961490874644369\n",
      "Training loss in direct  step: 7000 0.00039614501292817295\n",
      "Training loss in direct  step: 7020 0.0003961412003263831\n",
      "Training loss in direct  step: 7040 0.00039613814442418516\n",
      "Training loss in direct  step: 7060 0.0003961331385653466\n",
      "Training loss in direct  step: 7080 0.0003961303737014532\n",
      "Training loss in direct  step: 7100 0.00039612650289200246\n",
      "Training loss in direct  step: 7120 0.0003961219626944512\n",
      "Training loss in direct  step: 7140 0.00039611823740415275\n",
      "Training loss in direct  step: 7160 0.0003961147740483284\n",
      "Training loss in direct  step: 7180 0.00039611183456145227\n",
      "Training loss in direct  step: 7200 0.00039610802195966244\n",
      "Training loss in direct  step: 7220 0.0003961041511502117\n",
      "Training loss in direct  step: 7240 0.0003961008333135396\n",
      "Training loss in direct  step: 7260 0.0003960969334002584\n",
      "Training loss in direct  step: 7280 0.0003960929170716554\n",
      "Training loss in direct  step: 7300 0.000396088813431561\n",
      "Training loss in direct  step: 7320 0.0003960848553106189\n",
      "Training loss in direct  step: 7340 0.00039608043152838945\n",
      "Training loss in direct  step: 7360 0.00039607647340744734\n",
      "Training loss in direct  step: 7380 0.00039607242797501385\n",
      "Training loss in direct  step: 7400 0.0003960685571655631\n",
      "Training loss in direct  step: 7420 0.00039606483187526464\n",
      "Training loss in direct  step: 7440 0.0003960611065849662\n",
      "Training loss in direct  step: 7460 0.00039605714846402407\n",
      "Training loss in direct  step: 7480 0.000396052811993286\n",
      "Training loss in direct  step: 7500 0.00039604835910722613\n",
      "Training loss in direct  step: 7520 0.00039604477933607996\n",
      "Training loss in direct  step: 7540 0.00039604067569598556\n",
      "Training loss in direct  step: 7560 0.00039603738696314394\n",
      "Training loss in direct  step: 7580 0.0003960326430387795\n",
      "Training loss in direct  step: 7600 0.0003960285393986851\n",
      "Training loss in direct  step: 7620 0.00039602385368198156\n",
      "Training loss in direct  step: 7640 0.0003960193716920912\n",
      "Training loss in direct  step: 7660 0.0003960150061175227\n",
      "Training loss in direct  step: 7680 0.00039601096068508923\n",
      "Training loss in direct  step: 7700 0.00039600624586455524\n",
      "Training loss in direct  step: 7720 0.0003960031608585268\n",
      "Training loss in direct  step: 7740 0.00039599885349161923\n",
      "Training loss in direct  step: 7760 0.00039599460433237255\n",
      "Training loss in direct  step: 7780 0.0003959898022003472\n",
      "Training loss in direct  step: 7800 0.0003959857567679137\n",
      "Training loss in direct  step: 7820 0.00039598054718226194\n",
      "Training loss in direct  step: 7840 0.00039597658906131983\n",
      "Training loss in direct  step: 7860 0.0003959716123063117\n",
      "Training loss in direct  step: 7880 0.0003959674504585564\n",
      "Training loss in direct  step: 7900 0.0003959627356380224\n",
      "Training loss in direct  step: 7920 0.00039595869020558894\n",
      "Training loss in direct  step: 7940 0.0003959538007620722\n",
      "Training loss in direct  step: 7960 0.0003959498426411301\n",
      "Training loss in direct  step: 7980 0.0003959451278205961\n",
      "Training loss in direct  step: 8000 0.00039594125701114535\n",
      "Training loss in direct  step: 8020 0.00039593613473698497\n",
      "Training loss in direct  step: 8040 0.0003959319437853992\n",
      "Training loss in direct  step: 8060 0.00039592719986103475\n",
      "Training loss in direct  step: 8080 0.0003959222522098571\n",
      "Training loss in direct  step: 8100 0.00039591683889739215\n",
      "Training loss in direct  step: 8120 0.00039591267704963684\n",
      "Training loss in direct  step: 8140 0.0003959077293984592\n",
      "Training loss in direct  step: 8160 0.0003959026944357902\n",
      "Training loss in direct  step: 8180 0.0003958976885769516\n",
      "Training loss in direct  step: 8200 0.000395893381210044\n",
      "Training loss in direct  step: 8220 0.0003958876768592745\n",
      "Training loss in direct  step: 8240 0.00039588252548128366\n",
      "Training loss in direct  step: 8260 0.00039587743231095374\n",
      "Training loss in direct  step: 8280 0.00039587292121723294\n",
      "Training loss in direct  step: 8300 0.00039586826460435987\n",
      "Training loss in direct  step: 8320 0.0003958635497838259\n",
      "Training loss in direct  step: 8340 0.0003958592133130878\n",
      "Training loss in direct  step: 8360 0.00039585449849255383\n",
      "Training loss in direct  step: 8380 0.00039585024933330715\n",
      "Training loss in direct  step: 8400 0.0003958451852668077\n",
      "Training loss in direct  step: 8420 0.0003958401794079691\n",
      "Training loss in direct  step: 8440 0.00039583659963682294\n",
      "Training loss in direct  step: 8460 0.00039583275793120265\n",
      "Training loss in direct  step: 8480 0.0003958277520723641\n",
      "Training loss in direct  step: 8500 0.00039582393947057426\n",
      "Training loss in direct  step: 8520 0.00039581957389600575\n",
      "Training loss in direct  step: 8540 0.0003958155866712332\n",
      "Training loss in direct  step: 8560 0.0003958112793043256\n",
      "Training loss in direct  step: 8580 0.00039580691372975707\n",
      "Training loss in direct  step: 8600 0.00039580301381647587\n",
      "Training loss in direct  step: 8620 0.00039579812437295914\n",
      "Training loss in direct  step: 8640 0.00039579361327923834\n",
      "Training loss in direct  step: 8660 0.00039578869473189116\n",
      "Training loss in direct  step: 8680 0.00039578397991135716\n",
      "Training loss in direct  step: 8700 0.0003957793232984841\n",
      "Training loss in direct  step: 8720 0.0003957744629587978\n",
      "Training loss in direct  step: 8740 0.0003957688168156892\n",
      "Training loss in direct  step: 8760 0.0003957635199185461\n",
      "Training loss in direct  step: 8780 0.0003957583103328943\n",
      "Training loss in direct  step: 8800 0.0003957527515012771\n",
      "Training loss in direct  step: 8820 0.0003957474255003035\n",
      "Training loss in direct  step: 8840 0.0003957422450184822\n",
      "Training loss in direct  step: 8860 0.00039573718095198274\n",
      "Training loss in direct  step: 8880 0.00039573200047016144\n",
      "Training loss in direct  step: 8900 0.0003957254521083087\n",
      "Training loss in direct  step: 8920 0.00039572000969201326\n",
      "Training loss in direct  step: 8940 0.0003957150911446661\n",
      "Training loss in direct  step: 8960 0.0003957096196245402\n",
      "Training loss in direct  step: 8980 0.00039570455555804074\n",
      "Training loss in direct  step: 9000 0.00039569943328388035\n",
      "Training loss in direct  step: 9020 0.0003956940781790763\n",
      "Training loss in direct  step: 9040 0.00039568857755512\n",
      "Training loss in direct  step: 9060 0.0003956833970732987\n",
      "Training loss in direct  step: 9080 0.0003956790023948997\n",
      "Training loss in direct  step: 9100 0.00039567321073263884\n",
      "Training loss in direct  step: 9120 0.00039566896157339215\n",
      "Training loss in direct  step: 9140 0.00039566392661072314\n",
      "Training loss in direct  step: 9160 0.00039566043415106833\n",
      "Training loss in direct  step: 9180 0.00039565644692629576\n",
      "Training loss in direct  step: 9200 0.00039565152837894857\n",
      "Training loss in direct  step: 9220 0.0003956470754928887\n",
      "Training loss in direct  step: 9240 0.00039564340841025114\n",
      "Training loss in direct  step: 9260 0.00039563930477015674\n",
      "Training loss in direct  step: 9280 0.00039563514292240143\n",
      "Training loss in direct  step: 9300 0.0003956308646593243\n",
      "Training loss in direct  step: 9320 0.00039562786696478724\n",
      "Training loss in direct  step: 9340 0.00039562437450513244\n",
      "Training loss in direct  step: 9360 0.0003956215805374086\n",
      "Training loss in direct  step: 9380 0.000395618291804567\n",
      "Training loss in direct  step: 9400 0.00039561488665640354\n",
      "Training loss in direct  step: 9420 0.0003956120344810188\n",
      "Training loss in direct  step: 9440 0.00039560848381370306\n",
      "Training loss in direct  step: 9460 0.0003956055734306574\n",
      "Training loss in direct  step: 9480 0.0003956019936595112\n",
      "Training loss in direct  step: 9500 0.00039559893775731325\n",
      "Training loss in direct  step: 9520 0.0003955951251555234\n",
      "Training loss in direct  step: 9540 0.0003955912252422422\n",
      "Training loss in direct  step: 9560 0.000395587005186826\n",
      "Training loss in direct  step: 9580 0.0003955832216888666\n",
      "Training loss in direct  step: 9600 0.0003955787396989763\n",
      "Training loss in direct  step: 9620 0.00039557451964356005\n",
      "Training loss in direct  step: 9640 0.0003955695137847215\n",
      "Training loss in direct  step: 9660 0.0003955642750952393\n",
      "Training loss in direct  step: 9680 0.00039555877447128296\n",
      "Training loss in direct  step: 9700 0.0003955531574320048\n",
      "Training loss in direct  step: 9720 0.0003955478605348617\n",
      "Training loss in direct  step: 9740 0.00039554224349558353\n",
      "Training loss in direct  step: 9760 0.00039553651004098356\n",
      "Training loss in direct  step: 9780 0.00039553112583234906\n",
      "Training loss in direct  step: 9800 0.00039552594535052776\n",
      "Training loss in direct  step: 9820 0.00039552018279209733\n",
      "Training loss in direct  step: 9840 0.0003955135471187532\n",
      "Training loss in direct  step: 9860 0.0003955076972488314\n",
      "Training loss in direct  step: 9880 0.0003955023130401969\n",
      "Training loss in direct  step: 9900 0.00039549602661281824\n",
      "Training loss in direct  step: 9920 0.0003954906133003533\n",
      "Training loss in direct  step: 9940 0.00039548546192236245\n",
      "Training loss in direct  step: 9960 0.00039547926280647516\n",
      "Training loss in direct  step: 9980 0.0003954739077016711\n",
      "Training loss in direct  step: 10000 0.00039546817424707115\n",
      "Training loss in direct  step: 10020 0.0003954621497541666\n",
      "Training loss in direct  step: 10040 0.0003954566200263798\n",
      "Training loss in direct  step: 10060 0.0003954513231292367\n",
      "Training loss in direct  step: 10080 0.0003954453859478235\n",
      "Training loss in direct  step: 10100 0.00039543985622003675\n",
      "Training loss in direct  step: 10120 0.00039543441380374134\n",
      "Training loss in direct  step: 10140 0.00039542868034914136\n",
      "Training loss in direct  step: 10160 0.0003954242274630815\n",
      "Training loss in direct  step: 10180 0.00039541878504678607\n",
      "Training loss in direct  step: 10200 0.00039541375008411705\n",
      "Training loss in direct  step: 10220 0.000395408394979313\n",
      "Training loss in direct  step: 10240 0.0003954029525630176\n",
      "Training loss in direct  step: 10260 0.0003953981795348227\n",
      "Training loss in direct  step: 10280 0.00039539282443001866\n",
      "Training loss in direct  step: 10300 0.0003953883715439588\n",
      "Training loss in direct  step: 10320 0.00039538260898552835\n",
      "Training loss in direct  step: 10340 0.0003953776613343507\n",
      "Training loss in direct  step: 10360 0.000395373092032969\n",
      "Training loss in direct  step: 10380 0.0003953682316932827\n",
      "Training loss in direct  step: 10400 0.0003953634877689183\n",
      "Training loss in direct  step: 10420 0.0003953595587518066\n",
      "Training loss in direct  step: 10440 0.00039535530959255993\n",
      "Training loss in direct  step: 10460 0.0003953514387831092\n",
      "Training loss in direct  step: 10480 0.0003953469858970493\n",
      "Training loss in direct  step: 10500 0.0003953438135795295\n",
      "Training loss in direct  step: 10520 0.0003953396517317742\n",
      "Training loss in direct  step: 10540 0.0003953364212065935\n",
      "Training loss in direct  step: 10560 0.0003953324630856514\n",
      "Training loss in direct  step: 10580 0.0003953292325604707\n",
      "Training loss in direct  step: 10600 0.00039532530354335904\n",
      "Training loss in direct  step: 10620 0.0003953210252802819\n",
      "Training loss in direct  step: 10640 0.00039531729998998344\n",
      "Training loss in direct  step: 10660 0.00039531345828436315\n",
      "Training loss in direct  step: 10680 0.00039530970389023423\n",
      "Training loss in direct  step: 10700 0.0003953055420424789\n",
      "Training loss in direct  step: 10720 0.0003953009145334363\n",
      "Training loss in direct  step: 10740 0.00039529718924313784\n",
      "Training loss in direct  step: 10760 0.0003952931147068739\n",
      "Training loss in direct  step: 10780 0.00039528883644379675\n",
      "Training loss in direct  step: 10800 0.00039528473280370235\n",
      "Training loss in direct  step: 10820 0.0003952808037865907\n",
      "Training loss in direct  step: 10840 0.0003952766128350049\n",
      "Training loss in direct  step: 10860 0.00039527309127151966\n",
      "Training loss in direct  step: 10880 0.0003952686965931207\n",
      "Training loss in direct  step: 10900 0.0003952647384721786\n",
      "Training loss in direct  step: 10920 0.00039526046020910144\n",
      "Training loss in direct  step: 10940 0.0003952555707655847\n",
      "Training loss in direct  step: 10960 0.0003952516126446426\n",
      "Training loss in direct  step: 10980 0.00039524713065475225\n",
      "Training loss in direct  step: 11000 0.00039524221210740507\n",
      "Training loss in direct  step: 11020 0.00039523691521026194\n",
      "Training loss in direct  step: 11040 0.0003952324332203716\n",
      "Training loss in direct  step: 11060 0.0003952271363232285\n",
      "Training loss in direct  step: 11080 0.0003952228289563209\n",
      "Training loss in direct  step: 11100 0.00039521686267107725\n",
      "Training loss in direct  step: 11120 0.00039521249709650874\n",
      "Training loss in direct  step: 11140 0.00039520731661468744\n",
      "Training loss in direct  step: 11160 0.0003952027764171362\n",
      "Training loss in direct  step: 11180 0.00039519756683148444\n",
      "Training loss in direct  step: 11200 0.00039519291021861136\n",
      "Training loss in direct  step: 11220 0.00039518860285170376\n",
      "Training loss in direct  step: 11240 0.0003951848775614053\n",
      "Training loss in direct  step: 11260 0.0003951801627408713\n",
      "Training loss in direct  step: 11280 0.00039517629193142056\n",
      "Training loss in direct  step: 11300 0.0003951721591874957\n",
      "Training loss in direct  step: 11320 0.00039516837568953633\n",
      "Training loss in direct  step: 11340 0.0003951634862460196\n",
      "Training loss in direct  step: 11360 0.0003951609251089394\n",
      "Training loss in direct  step: 11380 0.00039515626849606633\n",
      "Training loss in direct  step: 11400 0.0003951527178287506\n",
      "Training loss in direct  step: 11420 0.0003951497783418745\n",
      "Training loss in direct  step: 11440 0.0003951458493247628\n",
      "Training loss in direct  step: 11460 0.0003951433172915131\n",
      "Training loss in direct  step: 11480 0.00039514017407782376\n",
      "Training loss in direct  step: 11500 0.00039513647789135575\n",
      "Training loss in direct  step: 11520 0.00039513391675427556\n",
      "Training loss in direct  step: 11540 0.0003951305116061121\n",
      "Training loss in direct  step: 11560 0.00039512745570391417\n",
      "Training loss in direct  step: 11580 0.00039512437069788575\n",
      "Training loss in direct  step: 11600 0.00039512087823823094\n",
      "Training loss in direct  step: 11620 0.0003951176186092198\n",
      "Training loss in direct  step: 11640 0.0003951145336031914\n",
      "Training loss in direct  step: 11660 0.00039511144859716296\n",
      "Training loss in direct  step: 11680 0.00039510868373326957\n",
      "Training loss in direct  step: 11700 0.00039510626811534166\n",
      "Training loss in direct  step: 11720 0.00039510385249741375\n",
      "Training loss in direct  step: 11740 0.0003951007965952158\n",
      "Training loss in direct  step: 11760 0.00039509826456196606\n",
      "Training loss in direct  step: 11780 0.0003950953541789204\n",
      "Training loss in direct  step: 11800 0.0003950923273805529\n",
      "Training loss in direct  step: 11820 0.0003950890386477113\n",
      "Training loss in direct  step: 11840 0.00039508743793703616\n",
      "Training loss in direct  step: 11860 0.0003950847021769732\n",
      "Training loss in direct  step: 11880 0.00039508065674453974\n",
      "Training loss in direct  step: 11900 0.0003950779791921377\n",
      "Training loss in direct  step: 11920 0.00039507492328993976\n",
      "Training loss in direct  step: 11940 0.0003950718673877418\n",
      "Training loss in direct  step: 11960 0.00039506860775873065\n",
      "Training loss in direct  step: 11980 0.00039506505709141493\n",
      "Training loss in direct  step: 12000 0.000395061302697286\n",
      "Training loss in direct  step: 12020 0.00039505702443420887\n",
      "Training loss in direct  step: 12040 0.0003950539685320109\n",
      "Training loss in direct  step: 12060 0.00039504922460764647\n",
      "Training loss in direct  step: 12080 0.00039504512096755207\n",
      "Training loss in direct  step: 12100 0.0003950410464312881\n",
      "Training loss in direct  step: 12120 0.0003950373793486506\n",
      "Training loss in direct  step: 12140 0.0003950321988668293\n",
      "Training loss in direct  step: 12160 0.00039502783329226077\n",
      "Training loss in direct  step: 12180 0.00039502314757555723\n",
      "Training loss in direct  step: 12200 0.00039501956780441105\n",
      "Training loss in direct  step: 12220 0.0003950151149183512\n",
      "Training loss in direct  step: 12240 0.00039501034189015627\n",
      "Training loss in direct  step: 12260 0.0003950061509385705\n",
      "Training loss in direct  step: 12280 0.00039500207640230656\n",
      "Training loss in direct  step: 12300 0.00039499669219367206\n",
      "Training loss in direct  step: 12320 0.0003949919482693076\n",
      "Training loss in direct  step: 12340 0.0003949867095798254\n",
      "Training loss in direct  step: 12360 0.00039498216938227415\n",
      "Training loss in direct  step: 12380 0.00039497771649621427\n",
      "Training loss in direct  step: 12400 0.0003949725942220539\n",
      "Training loss in direct  step: 12420 0.0003949674137402326\n",
      "Training loss in direct  step: 12440 0.0003949628444388509\n",
      "Training loss in direct  step: 12460 0.0003949579840991646\n",
      "Training loss in direct  step: 12480 0.0003949543461203575\n",
      "Training loss in direct  step: 12500 0.0003949497186113149\n",
      "Training loss in direct  step: 12520 0.00039494605152867734\n",
      "Training loss in direct  step: 12540 0.0003949416568502784\n",
      "Training loss in direct  step: 12560 0.0003949381352867931\n",
      "Training loss in direct  step: 12580 0.0003949341189581901\n",
      "Training loss in direct  step: 12600 0.0003949300153180957\n",
      "Training loss in direct  step: 12620 0.0003949256206396967\n",
      "Training loss in direct  step: 12640 0.0003949223319068551\n",
      "Training loss in direct  step: 12660 0.00039491872303187847\n",
      "Training loss in direct  step: 12680 0.0003949150559492409\n",
      "Training loss in direct  step: 12700 0.0003949118545278907\n",
      "Training loss in direct  step: 12720 0.0003949078673031181\n",
      "Training loss in direct  step: 12740 0.0003949049860239029\n",
      "Training loss in direct  step: 12760 0.000394901551771909\n",
      "Training loss in direct  step: 12780 0.00039489808841608465\n",
      "Training loss in direct  step: 12800 0.00039489500341005623\n",
      "Training loss in direct  step: 12820 0.00039489142363891006\n",
      "Training loss in direct  step: 12840 0.00039488787297159433\n",
      "Training loss in direct  step: 12860 0.00039488347829319537\n",
      "Training loss in direct  step: 12880 0.0003948792873416096\n",
      "Training loss in direct  step: 12900 0.00039487576577812433\n",
      "Training loss in direct  step: 12920 0.00039487131289206445\n",
      "Training loss in direct  step: 12940 0.00039486627792939544\n",
      "Training loss in direct  step: 12960 0.00039486237801611423\n",
      "Training loss in direct  step: 12980 0.00039485807064920664\n",
      "Training loss in direct  step: 13000 0.00039485387969762087\n",
      "Training loss in direct  step: 13020 0.0003948487574234605\n",
      "Training loss in direct  step: 13040 0.00039484479930251837\n",
      "Training loss in direct  step: 13060 0.00039484084118157625\n",
      "Training loss in direct  step: 13080 0.00039483699947595596\n",
      "Training loss in direct  step: 13100 0.00039483277942053974\n",
      "Training loss in direct  step: 13120 0.0003948288504034281\n",
      "Training loss in direct  step: 13140 0.00039482515421696007\n",
      "Training loss in direct  step: 13160 0.00039482099236920476\n",
      "Training loss in direct  step: 13180 0.0003948161320295185\n",
      "Training loss in direct  step: 13200 0.00039481199928559363\n",
      "Training loss in direct  step: 13220 0.00039480821578763425\n",
      "Training loss in direct  step: 13240 0.0003948048979509622\n",
      "Training loss in direct  step: 13260 0.00039480201667174697\n",
      "Training loss in direct  step: 13280 0.00039479878614656627\n",
      "Training loss in direct  step: 13300 0.000394794944440946\n",
      "Training loss in direct  step: 13320 0.00039479194674640894\n",
      "Training loss in direct  step: 13340 0.00039478775579482317\n",
      "Training loss in direct  step: 13360 0.00039478502003476024\n",
      "Training loss in direct  step: 13380 0.0003947822842746973\n",
      "Training loss in direct  step: 13400 0.0003947796067222953\n",
      "Training loss in direct  step: 13420 0.0003947772493120283\n",
      "Training loss in direct  step: 13440 0.0003947752411477268\n",
      "Training loss in direct  step: 13460 0.00039477244718000293\n",
      "Training loss in direct  step: 13480 0.0003947693039663136\n",
      "Training loss in direct  step: 13500 0.00039476691745221615\n",
      "Training loss in direct  step: 13520 0.0003947627265006304\n",
      "Training loss in direct  step: 13540 0.00039476066012866795\n",
      "Training loss in direct  step: 13560 0.0003947567311115563\n",
      "Training loss in direct  step: 13580 0.0003947547811549157\n",
      "Training loss in direct  step: 13600 0.0003947512886952609\n",
      "Training loss in direct  step: 13620 0.00039474706863984466\n",
      "Training loss in direct  step: 13640 0.0003947441291529685\n",
      "Training loss in direct  step: 13660 0.0003947403165511787\n",
      "Training loss in direct  step: 13680 0.0003947359218727797\n",
      "Training loss in direct  step: 13700 0.00039473167271353304\n",
      "Training loss in direct  step: 13720 0.0003947281511500478\n",
      "Training loss in direct  step: 13740 0.00039472448406741023\n",
      "Training loss in direct  step: 13760 0.0003947208751924336\n",
      "Training loss in direct  step: 13780 0.00039471648051403463\n",
      "Training loss in direct  step: 13800 0.00039471275522373617\n",
      "Training loss in direct  step: 13820 0.00039470737101510167\n",
      "Training loss in direct  step: 13840 0.0003947030636481941\n",
      "Training loss in direct  step: 13860 0.0003946993383578956\n",
      "Training loss in direct  step: 13880 0.0003946954384446144\n",
      "Training loss in direct  step: 13900 0.00039469145121984184\n",
      "Training loss in direct  step: 13920 0.00039468787144869566\n",
      "Training loss in direct  step: 13940 0.00039468376780860126\n",
      "Training loss in direct  step: 13960 0.00039467946044169366\n",
      "Training loss in direct  step: 13980 0.00039467503665946424\n",
      "Training loss in direct  step: 14000 0.00039467058377340436\n",
      "Training loss in direct  step: 14020 0.00039466601447202265\n",
      "Training loss in direct  step: 14040 0.00039466138696298003\n",
      "Training loss in direct  step: 14060 0.0003946571669075638\n",
      "Training loss in direct  step: 14080 0.0003946536162402481\n",
      "Training loss in direct  step: 14100 0.0003946486976929009\n",
      "Training loss in direct  step: 14120 0.0003946457291021943\n",
      "Training loss in direct  step: 14140 0.00039464220753870904\n",
      "Training loss in direct  step: 14160 0.00039463804569095373\n",
      "Training loss in direct  step: 14180 0.0003946336219087243\n",
      "Training loss in direct  step: 14200 0.0003946287324652076\n",
      "Training loss in direct  step: 14220 0.0003946251526940614\n",
      "Training loss in direct  step: 14240 0.0003946205251850188\n",
      "Training loss in direct  step: 14260 0.0003946154611185193\n",
      "Training loss in direct  step: 14280 0.00039461153210140765\n",
      "Training loss in direct  step: 14300 0.0003946069919038564\n",
      "Training loss in direct  step: 14320 0.00039460239349864423\n",
      "Training loss in direct  step: 14340 0.0003945966891478747\n",
      "Training loss in direct  step: 14360 0.0003945924108847976\n",
      "Training loss in direct  step: 14380 0.00039458711398765445\n",
      "Training loss in direct  step: 14400 0.00039458286482840776\n",
      "Training loss in direct  step: 14420 0.00039457797538489103\n",
      "Training loss in direct  step: 14440 0.0003945721546187997\n",
      "Training loss in direct  step: 14460 0.0003945677599404007\n",
      "Training loss in direct  step: 14480 0.00039456301601603627\n",
      "Training loss in direct  step: 14500 0.00039455815567635\n",
      "Training loss in direct  step: 14520 0.0003945535281673074\n",
      "Training loss in direct  step: 14540 0.0003945484641008079\n",
      "Training loss in direct  step: 14560 0.00039454406942240894\n",
      "Training loss in direct  step: 14580 0.00039453894714824855\n",
      "Training loss in direct  step: 14600 0.0003945340868085623\n",
      "Training loss in direct  step: 14620 0.00039452884811908007\n",
      "Training loss in direct  step: 14640 0.00039452416240237653\n",
      "Training loss in direct  step: 14660 0.0003945187199860811\n",
      "Training loss in direct  step: 14680 0.00039451426710002124\n",
      "Training loss in direct  step: 14700 0.0003945089119952172\n",
      "Training loss in direct  step: 14720 0.0003945048083551228\n",
      "Training loss in direct  step: 14740 0.0003944995114579797\n",
      "Training loss in direct  step: 14760 0.00039449494215659797\n",
      "Training loss in direct  step: 14780 0.00039449031464755535\n",
      "Training loss in direct  step: 14800 0.00039448478491976857\n",
      "Training loss in direct  step: 14820 0.00039447974995709956\n",
      "Training loss in direct  step: 14840 0.00039447436574846506\n",
      "Training loss in direct  step: 14860 0.00039446994196623564\n",
      "Training loss in direct  step: 14880 0.00039446481969207525\n",
      "Training loss in direct  step: 14900 0.00039445923175662756\n",
      "Training loss in direct  step: 14920 0.0003944540803786367\n",
      "Training loss in direct  step: 14940 0.00039444875437766314\n",
      "Training loss in direct  step: 14960 0.0003944437485188246\n",
      "Training loss in direct  step: 14980 0.00039443804416805506\n",
      "Training loss in direct  step: 15000 0.0003944327763747424\n",
      "Training loss in direct  step: 15020 0.00039442747947759926\n",
      "Training loss in direct  step: 15040 0.0003944223280996084\n",
      "Training loss in direct  step: 15060 0.0003944162745028734\n",
      "Training loss in direct  step: 15080 0.0003944106283597648\n",
      "Training loss in direct  step: 15100 0.00039440469117835164\n",
      "Training loss in direct  step: 15120 0.00039439904503524303\n",
      "Training loss in direct  step: 15140 0.00039439366082660854\n",
      "Training loss in direct  step: 15160 0.00039438821841031313\n",
      "Training loss in direct  step: 15180 0.0003943823685403913\n",
      "Training loss in direct  step: 15200 0.0003943769552279264\n",
      "Training loss in direct  step: 15220 0.0003943713672924787\n",
      "Training loss in direct  step: 15240 0.00039436601218767464\n",
      "Training loss in direct  step: 15260 0.00039436123915947974\n",
      "Training loss in direct  step: 15280 0.00039435632061213255\n",
      "Training loss in direct  step: 15300 0.0003943504416383803\n",
      "Training loss in direct  step: 15320 0.00039434561040252447\n",
      "Training loss in direct  step: 15340 0.0003943406918551773\n",
      "Training loss in direct  step: 15360 0.0003943356859963387\n",
      "Training loss in direct  step: 15380 0.0003943303890991956\n",
      "Training loss in direct  step: 15400 0.00039432442281395197\n",
      "Training loss in direct  step: 15420 0.0003943194169551134\n",
      "Training loss in direct  step: 15440 0.0003943144402001053\n",
      "Training loss in direct  step: 15460 0.0003943087940569967\n",
      "Training loss in direct  step: 15480 0.0003943041665479541\n",
      "Training loss in direct  step: 15500 0.00039429805474355817\n",
      "Training loss in direct  step: 15520 0.0003942928451579064\n",
      "Training loss in direct  step: 15540 0.00039428769377991557\n",
      "Training loss in direct  step: 15560 0.0003942819603253156\n",
      "Training loss in direct  step: 15580 0.0003942765179090202\n",
      "Training loss in direct  step: 15600 0.0003942707844544202\n",
      "Training loss in direct  step: 15620 0.0003942651383113116\n",
      "Training loss in direct  step: 15640 0.0003942592884413898\n",
      "Training loss in direct  step: 15660 0.0003942533512599766\n",
      "Training loss in direct  step: 15680 0.0003942479961551726\n",
      "Training loss in direct  step: 15700 0.00039424223359674215\n",
      "Training loss in direct  step: 15720 0.00039423658745363355\n",
      "Training loss in direct  step: 15740 0.00039423134876415133\n",
      "Training loss in direct  step: 15760 0.00039422535337507725\n",
      "Training loss in direct  step: 15780 0.00039421979454346\n",
      "Training loss in direct  step: 15800 0.00039421438123099506\n",
      "Training loss in direct  step: 15820 0.00039420888060703874\n",
      "Training loss in direct  step: 15840 0.0003942035837098956\n",
      "Training loss in direct  step: 15860 0.0003941980830859393\n",
      "Training loss in direct  step: 15880 0.00039419307722710073\n",
      "Training loss in direct  step: 15900 0.00039418719825334847\n",
      "Training loss in direct  step: 15920 0.0003941815230064094\n",
      "Training loss in direct  step: 15940 0.00039417616790160537\n",
      "Training loss in direct  step: 15960 0.0003941709583159536\n",
      "Training loss in direct  step: 15980 0.00039416601066477597\n",
      "Training loss in direct  step: 16000 0.000394160597352311\n",
      "Training loss in direct  step: 16020 0.00039415445644408464\n",
      "Training loss in direct  step: 16040 0.0003941490431316197\n",
      "Training loss in direct  step: 16060 0.00039414316415786743\n",
      "Training loss in direct  step: 16080 0.0003941370523534715\n",
      "Training loss in direct  step: 16100 0.0003941317554563284\n",
      "Training loss in direct  step: 16120 0.0003941269242204726\n",
      "Training loss in direct  step: 16140 0.0003941213944926858\n",
      "Training loss in direct  step: 16160 0.00039411598118022084\n",
      "Training loss in direct  step: 16180 0.00039411039324477315\n",
      "Training loss in direct  step: 16200 0.00039410486351698637\n",
      "Training loss in direct  step: 16220 0.00039409936289303005\n",
      "Training loss in direct  step: 16240 0.00039409383316524327\n",
      "Training loss in direct  step: 16260 0.0003940878377761692\n",
      "Training loss in direct  step: 16280 0.00039408274460583925\n",
      "Training loss in direct  step: 16300 0.0003940773895010352\n",
      "Training loss in direct  step: 16320 0.00039407305303029716\n",
      "Training loss in direct  step: 16340 0.00039406755240634084\n",
      "Training loss in direct  step: 16360 0.00039406277937814593\n",
      "Training loss in direct  step: 16380 0.00039405838469974697\n",
      "Training loss in direct  step: 16400 0.00039405387360602617\n",
      "Training loss in direct  step: 16420 0.0003940497408621013\n",
      "Training loss in direct  step: 16440 0.00039404421113431454\n",
      "Training loss in direct  step: 16460 0.0003940401948057115\n",
      "Training loss in direct  step: 16480 0.0003940350143238902\n",
      "Training loss in direct  step: 16500 0.00039403073606081307\n",
      "Training loss in direct  step: 16520 0.00039402607944794\n",
      "Training loss in direct  step: 16540 0.0003940219176001847\n",
      "Training loss in direct  step: 16560 0.0003940169117413461\n",
      "Training loss in direct  step: 16580 0.00039401219692081213\n",
      "Training loss in direct  step: 16600 0.00039400815148837864\n",
      "Training loss in direct  step: 16620 0.00039400282548740506\n",
      "Training loss in direct  step: 16640 0.00039399773231707513\n",
      "Training loss in direct  step: 16660 0.00039399295928888023\n",
      "Training loss in direct  step: 16680 0.0003939875168725848\n",
      "Training loss in direct  step: 16700 0.000393982685636729\n",
      "Training loss in direct  step: 16720 0.0003939773014280945\n",
      "Training loss in direct  step: 16740 0.00039397188811562955\n",
      "Training loss in direct  step: 16760 0.00039396717329509556\n",
      "Training loss in direct  step: 16780 0.00039396208012476563\n",
      "Training loss in direct  step: 16800 0.0003939564630854875\n",
      "Training loss in direct  step: 16820 0.0003939517482649535\n",
      "Training loss in direct  step: 16840 0.0003939469170290977\n",
      "Training loss in direct  step: 16860 0.00039394202758558095\n",
      "Training loss in direct  step: 16880 0.00039393737097270787\n",
      "Training loss in direct  step: 16900 0.00039393280167132616\n",
      "Training loss in direct  step: 16920 0.00039392802864313126\n",
      "Training loss in direct  step: 16940 0.00039392360486090183\n",
      "Training loss in direct  step: 16960 0.000393919792259112\n",
      "Training loss in direct  step: 16980 0.000393915077438578\n",
      "Training loss in direct  step: 17000 0.00039390884921886027\n",
      "Training loss in direct  step: 17020 0.0003939036978408694\n",
      "Training loss in direct  step: 17040 0.0003938986628782004\n",
      "Training loss in direct  step: 17060 0.00039389304583892226\n",
      "Training loss in direct  step: 17080 0.0003938876907341182\n",
      "Training loss in direct  step: 17100 0.0003938820445910096\n",
      "Training loss in direct  step: 17120 0.0003938762820325792\n",
      "Training loss in direct  step: 17140 0.00039387145079672337\n",
      "Training loss in direct  step: 17160 0.0003938657755497843\n",
      "Training loss in direct  step: 17180 0.0003938601294066757\n",
      "Training loss in direct  step: 17200 0.0003938541922252625\n",
      "Training loss in direct  step: 17220 0.00039384872070513666\n",
      "Training loss in direct  step: 17240 0.00039384368574246764\n",
      "Training loss in direct  step: 17260 0.0003938376030419022\n",
      "Training loss in direct  step: 17280 0.00039383239345625043\n",
      "Training loss in direct  step: 17300 0.00039382680552080274\n",
      "Training loss in direct  step: 17320 0.0003938213048968464\n",
      "Training loss in direct  step: 17340 0.00039381557144224644\n",
      "Training loss in direct  step: 17360 0.0003938103327527642\n",
      "Training loss in direct  step: 17380 0.0003938051813747734\n",
      "Training loss in direct  step: 17400 0.00039379895315505564\n",
      "Training loss in direct  step: 17420 0.00039379418012686074\n",
      "Training loss in direct  step: 17440 0.0003937888250220567\n",
      "Training loss in direct  step: 17460 0.000393784255720675\n",
      "Training loss in direct  step: 17480 0.000393778522266075\n",
      "Training loss in direct  step: 17500 0.00039377331268042326\n",
      "Training loss in direct  step: 17520 0.0003937681904062629\n",
      "Training loss in direct  step: 17540 0.00039376382483169436\n",
      "Training loss in direct  step: 17560 0.00039375905180349946\n",
      "Training loss in direct  step: 17580 0.0003937537840101868\n",
      "Training loss in direct  step: 17600 0.0003937497385777533\n",
      "Training loss in direct  step: 17620 0.0003937441506423056\n",
      "Training loss in direct  step: 17640 0.0003937389701604843\n",
      "Training loss in direct  step: 17660 0.00039373442996293306\n",
      "Training loss in direct  step: 17680 0.00039372872561216354\n",
      "Training loss in direct  step: 17700 0.0003937239816877991\n",
      "Training loss in direct  step: 17720 0.0003937185974791646\n",
      "Training loss in direct  step: 17740 0.000393713969970122\n",
      "Training loss in direct  step: 17760 0.00039370934246107936\n",
      "Training loss in direct  step: 17780 0.0003937048022635281\n",
      "Training loss in direct  step: 17800 0.0003937002329621464\n",
      "Training loss in direct  step: 17820 0.00039369516889564693\n",
      "Training loss in direct  step: 17840 0.00039369030855596066\n",
      "Training loss in direct  step: 17860 0.00039368553552776575\n",
      "Training loss in direct  step: 17880 0.0003936814027838409\n",
      "Training loss in direct  step: 17900 0.00039367598947137594\n",
      "Training loss in direct  step: 17920 0.0003936712455470115\n",
      "Training loss in direct  step: 17940 0.00039366690907627344\n",
      "Training loss in direct  step: 17960 0.0003936626890208572\n",
      "Training loss in direct  step: 17980 0.0003936579742003232\n",
      "Training loss in direct  step: 18000 0.0003936532011721283\n",
      "Training loss in direct  step: 18020 0.00039364860276691616\n",
      "Training loss in direct  step: 18040 0.0003936444118153304\n",
      "Training loss in direct  step: 18060 0.0003936399589292705\n",
      "Training loss in direct  step: 18080 0.000393634574720636\n",
      "Training loss in direct  step: 18100 0.00039362997631542385\n",
      "Training loss in direct  step: 18120 0.0003936257562600076\n",
      "Training loss in direct  step: 18140 0.0003936213906854391\n",
      "Training loss in direct  step: 18160 0.0003936178982257843\n",
      "Training loss in direct  step: 18180 0.00039361309609375894\n",
      "Training loss in direct  step: 18200 0.0003936092252843082\n",
      "Training loss in direct  step: 18220 0.0003936048597097397\n",
      "Training loss in direct  step: 18240 0.0003936004650313407\n",
      "Training loss in direct  step: 18260 0.00039359612856060266\n",
      "Training loss in direct  step: 18280 0.00039359278162010014\n",
      "Training loss in direct  step: 18300 0.00039358859066851437\n",
      "Training loss in direct  step: 18320 0.0003935841377824545\n",
      "Training loss in direct  step: 18340 0.0003935792774427682\n",
      "Training loss in direct  step: 18360 0.0003935749118681997\n",
      "Training loss in direct  step: 18380 0.0003935715649276972\n",
      "Training loss in direct  step: 18400 0.0003935661807190627\n",
      "Training loss in direct  step: 18420 0.00039356190245598555\n",
      "Training loss in direct  step: 18440 0.00039355785702355206\n",
      "Training loss in direct  step: 18460 0.0003935534623451531\n",
      "Training loss in direct  step: 18480 0.0003935500280931592\n",
      "Training loss in direct  step: 18500 0.0003935467393603176\n",
      "Training loss in direct  step: 18520 0.00039354298496618867\n",
      "Training loss in direct  step: 18540 0.0003935388522222638\n",
      "Training loss in direct  step: 18560 0.0003935355634894222\n",
      "Training loss in direct  step: 18580 0.00039353236206807196\n",
      "Training loss in direct  step: 18600 0.00039352872408926487\n",
      "Training loss in direct  step: 18620 0.0003935257263947278\n",
      "Training loss in direct  step: 18640 0.0003935218555852771\n",
      "Training loss in direct  step: 18660 0.00039351932355202734\n",
      "Training loss in direct  step: 18680 0.00039351623854599893\n",
      "Training loss in direct  step: 18700 0.00039351248415187\n",
      "Training loss in direct  step: 18720 0.00039350948645733297\n",
      "Training loss in direct  step: 18740 0.00039350680890493095\n",
      "Training loss in direct  step: 18760 0.0003935040149372071\n",
      "Training loss in direct  step: 18780 0.0003935015993192792\n",
      "Training loss in direct  step: 18800 0.0003934981650672853\n",
      "Training loss in direct  step: 18820 0.0003934958076570183\n",
      "Training loss in direct  step: 18840 0.0003934925189241767\n",
      "Training loss in direct  step: 18860 0.0003934902197215706\n",
      "Training loss in direct  step: 18880 0.00039348742575384676\n",
      "Training loss in direct  step: 18900 0.0003934853884857148\n",
      "Training loss in direct  step: 18920 0.0003934825072064996\n",
      "Training loss in direct  step: 18940 0.00039347982965409756\n",
      "Training loss in direct  step: 18960 0.0003934779670089483\n",
      "Training loss in direct  step: 18980 0.0003934752894565463\n",
      "Training loss in direct  step: 19000 0.00039347264100797474\n",
      "Training loss in direct  step: 19020 0.0003934699052479118\n",
      "Training loss in direct  step: 19040 0.00039346760604530573\n",
      "Training loss in direct  step: 19060 0.00039346545236185193\n",
      "Training loss in direct  step: 19080 0.00039346233825199306\n",
      "Training loss in direct  step: 19100 0.00039345986442640424\n",
      "Training loss in direct  step: 19120 0.000393456663005054\n",
      "Training loss in direct  step: 19140 0.0003934535488951951\n",
      "Training loss in direct  step: 19160 0.0003934512205887586\n",
      "Training loss in direct  step: 19180 0.00039344796095974743\n",
      "Training loss in direct  step: 19200 0.00039344534161500633\n",
      "Training loss in direct  step: 19220 0.0003934428095817566\n",
      "Training loss in direct  step: 19240 0.0003934393171221018\n",
      "Training loss in direct  step: 19260 0.00039343591197393835\n",
      "Training loss in direct  step: 19280 0.0003934335836675018\n",
      "Training loss in direct  step: 19300 0.00039342959644272923\n",
      "Training loss in direct  step: 19320 0.0003934265405405313\n",
      "Training loss in direct  step: 19340 0.00039342272793874145\n",
      "Training loss in direct  step: 19360 0.0003934191190637648\n",
      "Training loss in direct  step: 19380 0.0003934159758500755\n",
      "Training loss in direct  step: 19400 0.0003934129199478775\n",
      "Training loss in direct  step: 19420 0.00039340980583801866\n",
      "Training loss in direct  step: 19440 0.00039340637158602476\n",
      "Training loss in direct  step: 19460 0.00039340369403362274\n",
      "Training loss in direct  step: 19480 0.00039339877548627555\n",
      "Training loss in direct  step: 19500 0.00039339455543085933\n",
      "Training loss in direct  step: 19520 0.00039338902570307255\n",
      "Training loss in direct  step: 19540 0.0003933846310246736\n",
      "Training loss in direct  step: 19560 0.00039338000351563096\n",
      "Training loss in direct  step: 19580 0.00039337549242191017\n",
      "Training loss in direct  step: 19600 0.00039337045745924115\n",
      "Training loss in direct  step: 19620 0.00039336542249657214\n",
      "Training loss in direct  step: 19640 0.00039336082409136\n",
      "Training loss in direct  step: 19660 0.0003933558182325214\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-262-0e34311e381c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     39\u001B[0m         \u001B[0mmode\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;34m'direct'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0;31m#print(loss_direct)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m         \u001B[0mgrads_d\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgradient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss_direct\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrainable_weights\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m         \u001B[0;31m# Run one step of gradient descent by updating\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;31m# the value of the variables to minimize the loss.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mas/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36mgradient\u001B[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001B[0m\n\u001B[1;32m   1078\u001B[0m                           for x in nest.flatten(output_gradients)]\n\u001B[1;32m   1079\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1080\u001B[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001B[0m\u001B[1;32m   1081\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1082\u001B[0m         \u001B[0mflat_targets\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mas/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py\u001B[0m in \u001B[0;36mimperative_grad\u001B[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001B[0m\n\u001B[1;32m     69\u001B[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001B[1;32m     70\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 71\u001B[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001B[0m\u001B[1;32m     72\u001B[0m       \u001B[0mtape\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tape\u001B[0m\u001B[0;34m,\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     73\u001B[0m       \u001B[0mtarget\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mas/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py\u001B[0m in \u001B[0;36m_gradient_function\u001B[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001B[0m\n\u001B[1;32m    160\u001B[0m       \u001B[0mgradient_name_scope\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0mforward_pass_name_scope\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"/\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname_scope\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgradient_name_scope\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 162\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    163\u001B[0m   \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    164\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mgrad_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmock_op\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mout_grads\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mas/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py\u001B[0m in \u001B[0;36m_MatMulGrad\u001B[0;34m(op, grad)\u001B[0m\n\u001B[1;32m   1693\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mt_a\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mt_b\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1694\u001B[0m     \u001B[0mgrad_a\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmat_mul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtranspose_b\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1695\u001B[0;31m     \u001B[0mgrad_b\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmat_mul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtranspose_a\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1696\u001B[0m   \u001B[0;32melif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mt_a\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mt_b\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1697\u001B[0m     \u001B[0mgrad_a\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgen_math_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmat_mul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgrad\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mas/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36mmat_mul\u001B[0;34m(a, b, transpose_a, transpose_b, name)\u001B[0m\n\u001B[1;32m   5525\u001B[0m   \u001B[0;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5526\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 5527\u001B[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[1;32m   5528\u001B[0m         \u001B[0m_ctx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"MatMul\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"transpose_a\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtranspose_a\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"transpose_b\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   5529\u001B[0m         transpose_b)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def bounday_fct(x):\n",
    "    return np.sin(x*(np.pi))\n",
    "N=100\n",
    "#model.load_weights('checkpoints/lap_energy.h5')\n",
    "#model = tf.keras.models.load_model('checkpoints/lap_variational_almost.h5')\n",
    "x_len = np.linspace(0,1,N).reshape((N,1))\n",
    "#x_len = tf.convert_to_tensor(x_len)\n",
    "#model_loss = pde_fluct\n",
    "x_tensor = tf.convert_to_tensor(x_len, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "start = 1\n",
    "\n",
    "for step in range(0,65000):\n",
    "\n",
    "# Open a GradientTape to record the operations run\n",
    "# during the forward pass, which enables auto-differentiation.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "\n",
    "        # Create tensor that you will watch\n",
    "        # f = -1\n",
    "        tape.watch(x_tensor)\n",
    "        # Feed forward\n",
    "        output = model(x_tensor, training=True)*bounday_fct(x_tensor)\n",
    "        y_x = tape.gradient(output,x_tensor)\n",
    "        #y_xx = tape.gradient(y_x,x_tensor)\n",
    "\n",
    "        # Gradient and the corresponding loss function\n",
    "        #o_x = tape.gradient(output, x_tensor)\n",
    "        loss_direct = (tf.reduce_mean(input_tensor=(0.5*y_x**2 + output)**2)\n",
    "                      #+ 100*tf.square(y_x[0]-1)\n",
    "                      + 0*tf.square(output[0])\n",
    "                      #+ 100*tf.square(output[0]-output[-1])\n",
    "                      )\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00095)\n",
    "    if True:\n",
    "        #print('strasight train')\n",
    "        mode ='direct'\n",
    "        #print(loss_direct)\n",
    "        grads_d = tape.gradient(loss_direct, model.trainable_weights)\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads_d, model.trainable_weights))\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "    best_loss = 100\n",
    "    # Log every 200 batches\n",
    "    if step % 20 == 0:\n",
    "        print(\n",
    "            \"Training loss in \"+ mode + \"  step:\",\n",
    "            step, float(loss_direct)\n",
    "        )\n",
    "        if best_loss < float(loss_value):\n",
    "            model.save_weights('checkpoints/lap_energy_best.h5')\n",
    "            best_loss = float(loss_value)\n",
    "\n",
    "            #plt.plot(x_len, model(x_len))\n",
    "            #plt.plot(x_len, np.exp(x_len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_38 (InputLayer)        [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 24)                48        \n",
      "_________________________________________________________________\n",
      "dense_158 (Dense)            (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 673\n",
      "Trainable params: 673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.save('checkpoints/quad_pinn_relu_sine_bound.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7f9515a129d0>]"
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAuFUlEQVR4nO3deVyVdd7/8deHXRBFBFERRAVB3JXMLCu3XCYzm5qxaappmsyf5l1Nm83c0zJNZfueZWXZTJPZ7pS7meWWYrmzCi4ICriAgshyvr8/ON03caMc5cB1ls/z8TgPONf1vTjvr8v14Vznuj6XGGNQSinlfXysDqCUUsoaWgCUUspLaQFQSikvpQVAKaW8lBYApZTyUloAlFLKSzlUAERknIhkiEi2iMxqYL2IyMv29dtFZJB9eZCIbBKRbSKyS0QerbNNuIisEJEs+9d2zpuWUkqpxjRaAETEF3gNGA8kA9eLSHK9YeOBBPtjKjDHvvw0MNIY0x8YAIwTkaH2dbOAVcaYBGCV/blSSqkW4sg7gCFAtjEmxxhTCSwAJtUbMwl439TaCISJSCf785P2Mf72h6mzzXz79/OBq5swD6WUUufIz4Ex0cCBOs/zgAsdGBMNFNjfQWwB4oHXjDE/2MdEGWMKAIwxBSLSobEgERERJi4uzoHISimlfrZly5ZiY0xk/eWOFABpYFn9/hFnHGOMqQEGiEgY8LmI9DHG7HTgdWt/sMhUag8rERsbS2pqqqObKqWUAkRkX0PLHTkElAfE1HneBcg/1zHGmOPAt8A4+6LDItLJHq4TUNjQixtj5hpjUowxKZGR/6eAKaWUOk+OFIDNQIKIdBORAGAKsKjemEXATfazgYYCJfbDOpH23/wRkVbAaCC9zjY327+/GfiyaVNRSil1Lho9BGSMqRaRO4BlgC8wzxizS0Sm2de/ASwGJgDZQDlwi33zTsB8++cAPsBCY8xX9nWzgYUiciuwH7jOedNSSinVGHGndtApKSlGPwNQSqlzIyJbjDEp9ZfrlcBKKeWltAAopZSX0gKglFJeypHrAJRSQEVVDXnHyjlw9BRHyyopraii9FQ1NcbgK4KPQEigH+1bB9AuOIDOYUHEhAcT6OdrdXSlGqQFQKkGlFdWs3nvMbYdOM72vBJ25ZdQUFJxzj/HRyAmPJieUaEMjA1jYEw7+se0JThA/+sp6+m/QqXscopOsmTnIb7PKmLLvmNU1RhEoHtECBd2C6dHZGti2wfTpV0wEa0DaBPkT2iQH74+gjFQYwwnK6o5Wl7J0bJKDh47RU7RSfYUl7E7v5QVuw8DEODrw4XdwxmZ1IHRvaKICQ+2eObKW+lpoMqrFZ6o4PMfD/Kf7fnsPFgKQK9Obbg0IYJLEiIYEBNGaJC/U17raFklWw8cY332Eb7JKCSnqAyAQbFhTB4Yza/6dSY8JMApr6VUXWc6DVQLgPI6xhg25R7lnxv3sXTnIapthv5d2jKxf2cm9O1E57BWLZJj35Eyluw8xOc/HiTj8AkCfH2Y2L8zt1wcR5/oti2SQXkHLQDK69lshpVph3l1dTbb80poE+THtYNjuGFoLD0iW1uWyxjD7oJSPtp8gE+25FFeWcMFce24Y2QClyZEINJQr0WlHKcFQHktYwzLdh3mhRWZZBw+QWx4MNMu68HkgdG0CnCtM3RKTlXxyZY83vk+h/ySCgbEhHHX6AQu6xmphUCdNy0Ayiv9uP8YTy5OY/PeY3SPDGHmyHgm9uuMn69rXwJzurqGT7bk8frqPRw8foqL49vz379KplenNlZHU25IC4DyKkUnTvP417v5Yms+Ea0DuXtMAr9NiXH5HX99ldU2PvhhHy+tyqL0VBW/SYnh/nFJ+mGxOidaAJRXqLEZ/r1pP08vTed0lY3bL+vOtMt6EBLo3mc8Hy+v5JVvspm/fi+hQX787cpkJg+M1sNCyiFaAJTH21tcxr0fbyN13zGG9WjPY1f3sfTD3eaQcegEsz7bzk/7j3NJfARPXtNXryNQjdICoDyWMYZ//bCfJ75Ow89XeHhib349yHN/O7bZDB/8sI/ZS9LxEeGRq3pzjQfPVzXdmQqAe78vVl7vaFkl9yzcyuqMIoYnRPD0tf3o1LZlzuO3io+PcONFcVye2IE/L9zKPR9vY1X6YZ6Y3JewYP1sQDlOC4ByW5v3HmXmv3/iaFklj17Vm5su6upVvwXHhAezYOpFvPndHl5Ykcm2AyXM+f0g+nUJszqachPudUqEUtQe8nljzR6mzN1IoL8Pn00fxs3D4rxq5/8zXx9h+uXxLLz9IgCunbOBf27chzsd2lXW0QKg3MqpyhruXLCV2UvSGds7iq9mXqJtE4CBse34auYlDItvz9++2Mk9H2+joqrG6ljKxekhIOU28o+fYuo/U9mVX8p9YxOZfnkPr/yt/0zahQQw7+YLePmbLF5cmUVucRlv3jiYDqFBVkdTLkrfASi3sPNgCZNeW8fe4nLevimFGSPideffAB8f4a7RPXn9hkGkFZQy6dV17MovsTqWclFaAJTLW51RyG/e3ECAb+3x/lG9oqyO5PIm9O3EJ9OGAfDbNzfyfVaRxYmUK9ICoFzah5v286f5qXSLCOHz6cPoGRVqdSS30Se6LZ9Pv5gu7Vpxy7ub+fynPKsjKRejBUC5rNe/zebBz3ZwSXwEC2+/iA5t9Fj2uerYNoiF0y7igrhw7v5oG3O/22N1JOVCtAAol2OM4aml6Ty9NIOr+nfm7ZtT3L6Xj5XaBPnz3h8v4Ff9OvHE4nSeX56hp4kqQM8CUi7GZjM88p9dvL9hH7+7MJbHJvXB10c/7G2qQD9fXp4ykJAAX17+Jpuyyhr++1e99IN0L+fQOwARGSciGSKSLSKzGlgvIvKyff12ERlkXx4jIqtFJE1EdonInXW2eUREDorIVvtjgvOmpdyRzWb425c7eX/DPm6/tDuPX607f2fy9RFmX9OPPwyL4521ufz1i53YbPpOwJs1+g5ARHyB14AxQB6wWUQWGWN21xk2HkiwPy4E5ti/VgP3GGN+FJFQYIuIrKiz7QvGmGedNx3lrn7e+X/ww36mX96D+8Ym6m+nzcDHR3h4YjKtAnyZ8+0efAQem9RH/6y9lCOHgIYA2caYHAARWQBMAuoWgEnA+6b2wOJGEQkTkU7GmAKgAMAYc0JE0oDoetsqL2eM7vxbkohw/9hEbMbw5pocfER49Kre+mfuhRw5BBQNHKjzPM++7JzGiEgcMBD4oc7iO+yHjOaJSLuGXlxEpopIqoikFhXpucyexhjDP75O44Mf9vP/dOffYkSEWeOSuG14N97fsI/HvkrTD4a9kCMFoKH/jfX/pZx1jIi0Bj4F7jLGlNoXzwF6AAOofZfwXEMvboyZa4xJMcakREZGOhBXuZOXVmXxztpc/jAsjvt159+iRIS/TOjFLRfHMW9dLi+uzLI6kmphjhwCygNi6jzvAuQ7OkZE/Knd+X9gjPns5wHGmMM/fy8ibwFfnVNy5fbe/j6HF1dmcd3gLjx0ZbLu/C0gIjx0ZTInK6p5aVUWbVr5c+sl3ayOpVqII+8ANgMJItJNRAKAKcCiemMWATfZzwYaCpQYYwqk9n/0O0CaMeb5uhuISKc6TycDO897FsrtfLolj398ncaEvh2Z/et++OjZPpYREZ68pi/j+3Tksa92szD1QOMbKY/Q6DsAY0y1iNwBLAN8gXnGmF0iMs2+/g1gMTAByAbKgVvsm18M3AjsEJGt9mV/McYsBp4WkQHUHiraC9zupDkpF/dtRiEPfLqdi+Pb88JvB+ipni7Az9eHF6cM4OT8VB78bAeRrQMZkdTB6liqmek9gVWL2pFXwm/nbiCufQgf3T6U0CB/qyOpOspOV/PbuRvYU1jGgqlD6R8TZnUk5QRnuiewtoJQLebA0XJueW8T4SEBvHfLBbrzd0EhgX7M+8MFRIQG8Mf3NrO3uMzqSKoZaQFQLaLkVBW3vLeZqhrD/D8O0cZuLqxDaBDzbxmCzRj+8O4mjpVVWh1JNRMtAKrZVdXYmPHBj+w7UnuHqh6Rra2OpBrRPbI1b9+cQn5JBdP+tYXKapvVkVQz0AKgmpUxhoe+3Mna7GKemNyXod3bWx1JOWhw13CeubYfP+Qe5S+f79ALxTyQdgNVzerddXv5cNMBZozowXUpMY1voFzKpAHR5BSV8dKqLLpHhjD98nirIykn0gKgms3arGIeX5zG2N5R3DMm0eo46jzdNTqBnOIynl6aQc8OoYxO1ltyego9BKSaxb4jZcz494/ER7bmud8M0Au93JiI8My1/egT3Ya7PtpKduEJqyMpJ9ECoJzu5Olqbns/FRF466YUWuvdvNxekL8vc29MIcjfh9ve30LJqSqrIykn0AKgnMoYw/2fbCO78CSv/W4Qse2DrY6knKRzWCtev2EwB46Wc+eCn6jRm8m4PS0Ayqne/j6XxTsOMWt8EhfHR1gdRznZkG7hPHJVb77NKOKlVdo91N1pAVBOszHnCLOXpjO+T0duG97d6jiqmdxwYSy/HtSFl1dlsTq90Oo4qgm0ACinOFxawR3//omu7YN5+tp+2trZg4kI/7i6D706teHOBT+x/0i51ZHUedICoJqsusbGzA9/oryymjd/P1h7/HiBVgG+vPH7QQBM+9cWKqpqLE6kzocWANVkL67MYlPuUZ6Y3JeEqFCr46gW0rV9CC/8dgC7C0p57Cu9zbc70gKgmmRNZhGvfZvNlAtiuHpg/VtFK083qlcUt1/anQ9+2M9/ttW/UaBydVoA1Hk7VFLB3R9tJTEqlEeu6m11HGWRe8cmMig2jAc/20Guto92K1oA1HmpsRnu+ugnKqpqePV3gwjy97U6krKIv68Pr/xuEL4+wowPftTPA9yIFgB1Xt5Ys4eNOUd59KrexHfQ9s7eLjqsFc9d15/dBaXMXpJudRzlIC0A6pxt2XeM51dkMrF/Z64d3MXqOMpFjE6O4g/D4nhv/V6+ST9sdRzlAC0A6pyUVlRx54Kf6NQ2iMcn99Hz/dUvzBqfRFLHUO79eDuFpRVWx1GN0AKgzslDX+ykoKSCl6YMpI2e76/qCfL35ZXrB1JeWc2fF27Dpv2CXJoWAOWwRdvy+WJrPjNHxjO4azur4ygXlRAVykNX9mZtdjHz1uVaHUedhRYA5ZD846f47893MDA2jDtG6F2h1NldPySG0b2ieHppBhmH9P4BrkoLgGqUzWa49+NtVNsML/xmAH6++s9GnZ2IMPvXfWnTyo87F/zE6Wo9NdQV6f9k1ah31+9l/Z4jPHRlMnERIVbHUW4ionUgT/26H+mHTvD88kyr46gGOFQARGSciGSISLaIzGpgvYjIy/b120VkkH15jIisFpE0EdklInfW2SZcRFaISJb9qx5UdkHZhSd5amk6o3t14LcX6E3d1bkZ1SuK64fEMvf7HDbmHLE6jqqn0QIgIr7Aa8B4IBm4XkSS6w0bDyTYH1OBOfbl1cA9xphewFBgRp1tZwGrjDEJwCr7c+VCqmts3PPxNoIDfHnimr56yqc6L3+7shex4cHc98k2yk5XWx1H1eHIO4AhQLYxJscYUwksACbVGzMJeN/U2giEiUgnY0yBMeZHAGPMCSANiK6zzXz79/OBq5s2FeVsb36Xw7YDx/nH1X3oEBpkdRzlpoID/Hj2uv7kHTvFE4vTrI6j6nCkAEQDB+o8z+N/d+IOjxGROGAg8IN9UZQxpgDA/rWDw6lVs9udX8qLKzP5Vb9OXNmvs9VxlJu7IC6c24bXdg1dk1lkdRxl50gBaOh9f/2rO846RkRaA58CdxljSh2PByIyVURSRSS1qEj/4bSEqhob9368jbatAnhsUh+r4ygP8ecxPYnv0JoHPtlOyakqq+MoHCsAeUDdT/+6APUbf59xjIj4U7vz/8AY81mdMYdFpJN9TCegwZuLGmPmGmNSjDEpkZGRDsRVTfXGt3vYXVDKP67uQ3hIgNVxlIcI8vfluev6U3TytN5AxkU4UgA2Awki0k1EAoApwKJ6YxYBN9nPBhoKlBhjCqT2U8N3gDRjzPMNbHOz/fubgS/PexbKaTIOneDlb7K4sl8nxvXpaHUc5WH6x4Rx+6Xd+WRLHt9m6A3lrdZoATDGVAN3AMuo/RB3oTFml4hME5Fp9mGLgRwgG3gLmG5ffjFwIzBSRLbaHxPs62YDY0QkCxhjf64sVF1j475PthEa5M+jeoMX1Uz+a1QC8R1a8+BnOzhRoYeCrCTGuE+zppSUFJOammp1DI/1xpo9zF6SzivXD2Rif/3gVzWfH/cf49o565kyJJYnJve1Oo7HE5EtxpiU+sv1SmAFQG5xGc+vyOSK5Ciu7NfJ6jjKww2Kbcetl3Tj3z/sZ312sdVxvJYWAIXNZpj16XYC/Xx47Grt8a9axj1XJNItIoRZn+3gVKX2CrKCFgDFwtQD/JB7lL9O6EVUG73gS7WMIH9fnrymL/uPlvPiSu0VZAUtAF6usLSCxxenMbR7uPb6US1uaPf2XD8khre+z2FHXonVcbyOFgAv9/CiXZyutvHkNf300I+yxKzxvYhoHcgDn26nqsZmdRyvogXAi63YfZglOw9x56gEummbZ2WRtq38+fukPuwuKOWt73OsjuNVtAB4qbLT1Tz85U4So0KZeml3q+MoLzeuT0fG9e7ISyuz2H+k3Oo4XkMLgJd6fkUm+SUVPHFNH/z1Dl/KBTxyVW/8fX346xc7cKfrk9yZ/s/3QjsPlvDuulxuuDCWwV3DrY6jFAAd2wZx39hEvs8qZtG2+u3GVHPQAuBlamyGv3y+g/CQQO4fl2R1HKV+4fdDu9I/JozHvtrN8fJKq+N4PC0AXuaDH/axPa+EhyYm07aVv9VxlPoFXx/hicl9OFZexVNL062O4/G0AHiRwhMVPLM0g0viI5io7R6Ui+rduS1/vDiODzcdYMu+o1bH8WhaALzIE1+ncbraxt8n9dZz/pVLu2t0Tzq1DeKvn++kWq8NaDZaALzE+uxivtiaz7TLutM9srXVcZQ6q5BAPx6emEz6oRO8t36v1XE8lhYAL1BZbeNvX+4kNjyY6SPirY6jlEPG9u7IiMRIXliRSUHJKavjeCQtAF7gnbW57Ckq49FJvQny97U6jlIOEREevaoP1TbD3/+jt5BsDloAPFz+8VO8vCqLK5KjGJHYweo4Sp2T2PbBzBwZz5Kdh/gus8jqOB5HC4CHe/zrNGzG8Lcrk62OotR5ue3S7sS1D+aRRbs4Xa33DXAmLQAebG1WMV/vKGDGiHhiwoOtjqPUeQn08+WRq3qTU1zG29/nWh3Ho2gB8FCV1TYeXrSTru2DtdmbcnuXJ3ZgXO+OvPJNFgeP6wfCzqIFwEO9t772g99HJuoHv8oz/G1i7WHMv/9nl8VJPIcWAA9UWFrBSyuzGJXUgRFJ+sGv8gzRYa2YOTKBZbsO832WfiDsDFoAPNDspelU1egHv8rz/Gl4N7raPxCurNYrhJtKC4CH2bLvKJ/9eJA/De9GnN7lS3mYQD9fHroymT1FZczXK4SbTAuAB6mxGR5ZtJuObYKYoVf8Kg81qlcUI5M68NKqLApLK6yO49a0AHiQj1MPsONgCQ9OSCIk0M/qOEo1m4euTKay2sbsJdoyuikcKgAiMk5EMkQkW0RmNbBeRORl+/rtIjKozrp5IlIoIjvrbfOIiBwUka32x4SmT8d7lVZU8cyyDFK6tuOq/p2tjqNUs4qLCOFPw7vx2U8H+XH/MavjuK1GC4CI+AKvAeOBZOB6Ean/6eJ4IMH+mArMqbPuPWDcGX78C8aYAfbH4nPMrup4ZVUWR8sreXiitnpW3mHGiHii2gTy6KJd2Gx6D+Hz4cg7gCFAtjEmxxhTCSwAJtUbMwl439TaCISJSCcAY8x3gN7VoRntKTrJu+v28pvBMfTt0tbqOEq1iJBAP2aNT2JbXgmf/phndRy35EgBiAYO1HmeZ192rmMacof9kNE8EWnX0AARmSoiqSKSWlSk5/425PGv0wjy9+XesYlWR1GqRU3qH83A2DCeWprBiYoqq+O4HUcKQEPHE+q/33JkTH1zgB7AAKAAeK6hQcaYucaYFGNMSmRkZCM/0vusySzim/RC/mtUPJGhgVbHUapF+fgIj0zsTfHJ07y6OtvqOG7HkQKQB8TUed4FyD+PMb9gjDlsjKkxxtiAt6g91KTOQXWNjce+2k3X9sHcPCzO6jhKWaJ/TBjXDu7CvLW57C0uszqOW3GkAGwGEkSkm4gEAFOARfXGLAJusp8NNBQoMcYUnO2H/vwZgd1kYOeZxqqG/XvTfrILT/LXCb0I9NN+P8p73T82kQBfH55YnGZ1FLfSaAEwxlQDdwDLgDRgoTFml4hME5Fp9mGLgRwgm9rf5qf/vL2IfAhsABJFJE9EbrWvelpEdojIdmAEcLezJuUNSsqreGFFJhd1b8+Y5Cir4yhlqQ5tgpg+Ip7luw+zPrvY6jhuQ4xxn9OnUlJSTGpqqtUxXMJjX+1m3rpcvp45nOTObayOo5TlKqpqGP38GloH+vH1fw3H10dPh/6ZiGwxxqTUX65XAruhnKKTzF+/lykXxOjOXym7IH9f/jKhF+mHTrBg836r47gFLQBu6Mkl6QT6+fDnMXrap1J1je/TkSHdwnlueSalelpoo7QAuJkNe46wYvdhpo/Q0z6Vqk9EeOjKZI6VV/LaN3paaGO0ALgRm83wj693Ex3Wilsv6WZ1HKVcUp/otvx6UBfeXbeX/UfKrY7j0rQAuJHPfjrIrvxS7h+XqLd5VOos7hubiK+PMHupnhZ6NloA3ER5ZTXPLEtnQEyYdvtUqhFRbYKYdlkPFu84xOa92orsTLQAuIm3vsvlcOlp/vtXvbTbp1IOuO3SbnRsE8RjX+3WbqFnoAXADRSWVvDmd3uY0LcjKXHhVsdRyi0EB/hx/7hEtueVsGjbWTvTeC0tAG7ghZWZVNXYuH9sktVRlHIrVw+Ipm90W55emk5FVY3VcVyOFgAXl3n4BB9tPsCNQ+P0Ju9KnSMfH+EvE3qRX1LBO2tzrY7jcrQAuLgnF6cREujHzJF6k3elzsdFPdozulcUc77dQ/HJ01bHcSlaAFzYuuxiVmcUMXNkPO1CAqyOo5TbenBCEhVVNby4MtPqKC5FC4CLstkMj3+dRnRYK266KM7qOEq5tR6Rrbnhwlg+3HSA7MITVsdxGVoAXNQXWw+yu0Av+lLKWe4c3ZNgf19mL0m3OorL0ALggiqqanh2WQZ9o9sysZ9e9KWUM4SHBDB9RDwr0wrZsOeI1XFcghYAF/Te+r3kl1Tw4IQkfLSnuVJOc8vFcXRuG8QTi9P04jC0ALicY2WVvLY6m5FJHRjWI8LqOEp5lCB/X+4dm8iOgyX8Z7teHKYFwMW88k02ZaermTVeL/pSqjlcPSCa3p3b8PTSDK+/OEwLgAs5cLScf27cy3WDY+gZFWp1HKU80s8Xhx08for3N+y1Oo6ltAC4kGeXZ+DrI9w9pqfVUZTyaBfHR3B5YiSvfpPN8fJKq+NYRguAi9h5sIQvt+Zz6yXd6Ng2yOo4Snm8WeOTOHm6mle9+M5hWgBcxOwl6bQL9uf2y3pYHUUpr5DUsQ3XDu7C+xv2ceCod945TAuAC/gus4i12cXMHJlAmyB/q+Mo5TXuHtMTH5/aw6/eSAuAxWw2w+wl6cSEt+KGobFWx1HKq3RqW3t/7S+35rMjr8TqOC1OC4DFFm3LZ3dBKfdekUign7Z8UKqlTbusB+EhAcxemoYx3nVxmEMFQETGiUiGiGSLyKwG1ouIvGxfv11EBtVZN09ECkVkZ71twkVkhYhk2b+2a/p03Mvp6hqeXZ5B785ttOWDUhYJDfJn5sh41mUf4busYqvjtKhGC4CI+AKvAeOBZOB6EUmuN2w8kGB/TAXm1Fn3HjCugR89C1hljEkAVtmfe5UPNu4n79gpZo3Xlg9KWemGC7sSGx7M7CXpXtUiwpF3AEOAbGNMjjGmElgATKo3ZhLwvqm1EQgTkU4AxpjvgKMN/NxJwHz79/OBq88jv9sqrajilW+yuCQ+guEJkVbHUcqrBfj5cO/YRNIKSvli60Gr47QYRwpANHCgzvM8+7JzHVNflDGmAMD+tYMDWTzG3DU5HCuv4oFx2vJBKVdwZd9O9I1uy3PLM72mRYQjBaChYxP13yM5Mua8iMhUEUkVkdSioiJn/EjLFZZW8PbaHK7s14m+XdpaHUcpRW2LiAfHJ3Hw+Cn+uWGf1XFahCMFIA+IqfO8C1C/jZ4jY+o7/PNhIvvXwoYGGWPmGmNSjDEpkZGecajkpVVZVNcY7r0i0eooSqk6hsVHcFnPSF5dnU3JqSqr4zQ7RwrAZiBBRLqJSAAwBVhUb8wi4Cb72UBDgZKfD++cxSLgZvv3NwNfnkNut5VbXMaCzQe4fkgscREhVsdRStXzwLgkSiuqmPPtHqujNLtGC4Axphq4A1gGpAELjTG7RGSaiEyzD1sM5ADZwFvA9J+3F5EPgQ1Aoojkicit9lWzgTEikgWMsT/3eM8uzyDA14eZo+KtjqKUakBy5zZcPSCad9flUlByyuo4zcrPkUHGmMXU7uTrLnujzvcGmHGGba8/w/IjwCiHk3qAHXklfL29gJkj4+kQqg3flHJVfx7Tk6+3F/Diiiyeuraf1XGajV4J3IKeWlrb8G3qpd2tjqKUOouY8GBuvKgrH285QObhE1bHaTZaAFrI2qxi1mYXM2NEPKHa8E0plzdjRDwhAX48vdRzG8VpAWgBxhieWppOdFgrfj+0q9VxlFIOCA8JYNrlPViZdpjUvQ1dy+r+tAC0gMU7DrHjYAl3j+lJkL82fFPKXdxycRyRoYHMXpLukY3itAA0s6oaG88uz6BnVGsmD2zs4millCsJDvDjrtEJpO47xqq0Bi9VcmtaAJrZx6l55BaXcd/YJHy14ZtSbuc3KTF0jwjh6WXp1HhYozgtAM3oVGUNL63KZHDXdozu5VWtjpTyGP6+tY3iMg+f5NMf86yO41RaAJrR/A17OVx6mgfGJSGiv/0r5a7G9+lI/5gwXlzhWY3itAA0k5LyKl5fnc2IxEiGdAu3Oo5SqglEhAfGJZJfUuFRjeK0ADSTN7/bQ2lFNfeN1XbPSnmCYT0iuLRnJK99m01phWc0itMC0AwKSyuYty6XSQM6k9y5jdVxlFJOcv/YRI6XV/HmGs9oFKcFoBm8/E1tu+d7xmi7Z6U8SZ/otkwa0Jl31uZSWFphdZwm0wLgZHuLy1iwqbbdc2z7YKvjKKWc7J4xidTYDC+uyrI6SpNpAXCy51dk4q/tnpXyWLHtg/ndkFg+2nyA3OIyq+M0iRYAJ9qVX8Kibfn88ZI4bfeslAe7Y2QCgX4+PLvcvRvFaQFwomeWZdC2lT9TL+1hdRSlVDOKDA3kT5d04+vtBezIK7E6znnTAuAkP+Qc4duMIqZf3oO2rbTds1Ke7rZLuxMeEsBTS9OtjnLetAA4gTGGp5dlENUmkJuHxVkdRynVAkKD/JkxIp612cWszSq2Os550QLgBKvSCtmy7xh3jtJ2z0p5k98PjSU6rBVPLU3H5oaN4rQANFGNzfDMsgy6RYRwXUoXq+MopVpQoJ8vd4/pyY6DJSzeWWB1nHOmBaCJFm07SMbhE9xzRU/8ffWPUylvM3lgNIlRoTy7LIOqGpvVcc6J7rGaoLLaxnPLM+kT3YYJfTpZHUcpZQFfH+G+sYnsPVLOwtQDVsc5J1oAmuDDTfvJO3aK+8Ym4aM3e1HKa43q1YGUru14aWUWpyrdp120FoDzVHa6mle+yWJo93AuTYiwOo5SykIiwqzxSRSeOM28dblWx3GYFoDzNG9tLsUnK7lfb/ailAJS4sIZ3asDb6zZw/HySqvjOEQLwHk4VlbJ3O9yuCI5ikGx7ayOo5RyEfeNTeLk6Wpe/9Y92kU7VABEZJyIZIhItojMamC9iMjL9vXbRWRQY9uKyCMiclBEttofE5wzpeY3Z80eyiqruXestntWSv2vxI6hXDOwC++t30v+8VNWx2lUowVARHyB14DxQDJwvYgk1xs2HkiwP6YCcxzc9gVjzAD7Y3FTJ9MSCkpO8d76vUwe2IWeUaFWx1FKuZi7xySAgRdXZlodpVGOvAMYAmQbY3KMMZXAAmBSvTGTgPdNrY1AmIh0cnBbt/LSyiwwcNfoBKujKKVcUJd2wdx4UVc+2ZJH1uETVsc5K0cKQDRQ9+TWPPsyR8Y0tu0d9kNG80SkwYPpIjJVRFJFJLWoqMiBuM1nT9FJPt6Sxw1DY4kJ15u9KKUaNmNEPCEBfjyzzLXbRTtSABo6xaV+04szjTnbtnOAHsAAoAB4rqEXN8bMNcakGGNSIiMjHYjbfJ5bnkGQnw8zRujNXpRSZxYeEsDUS7uzfPdhtuw7ZnWcM3KkAOQBMXWedwHyHRxzxm2NMYeNMTXGGBvwFrWHi1zWtgPHWbzjEH8a3p2I1oFWx1FKubhbh3cjonUgTy1JxxjXbBTnSAHYDCSISDcRCQCmAIvqjVkE3GQ/G2goUGKMKTjbtvbPCH42GdjZxLk0q2eWZRAeEsCfhnezOopSyg0EB/hx5+gENu09yuqMQqvjNKjRAmCMqQbuAJYBacBCY8wuEZkmItPswxYDOUA2tb/NTz/btvZtnhaRHSKyHRgB3O28aTnX2qxi1mYXM2NEPKFBerMXpZRjplwQQ1z7YJ5emkGNC7aLFld9a9KQlJQUk5qa2qKvaYxh0mvrOHKyklX3XKb9/pVS5+Q/2/KZ+eFPPP+b/lwzyJqW8SKyxRiTUn+5XgnciCU7D7E9r4S7Rifozl8pdc5+1bcTfaLb8NzyTE5Xu1ajOC0AZ1FdY+PZZRkkdGhtWeVWSrk3Hx9h1rheHDx+in9t3G91nF/QAnAWH2/JI6e4jPvGJuKr7Z6VUufpkoQIhidE8Oo3WZRWVFkd539oATiDU5U1vLgyk0GxYYxJjrI6jlLKzT0wLolj5VXMXZNjdZT/oQXgDOZv2Mvh0tM8oO2elVJO0Ce6LRP7d+adtbkUllZYHQfQAtCgkvIqXl+dzYjESC7s3t7qOEopD3HvFT2pqrHx0qosq6MAWgAaNGfNHk6crua+sUlWR1FKeZCu7UO44cJYFmw+QE7RSavjaAGo71BJBe+uy+XqAdEkd25jdRyllIeZOSqBID8fl2gUpwWgnhdXZmIzhj+P6Wl1FKWUB4poHchtl3Znyc5D/Ljf2kZxWgDqyC48ycLUA/x+aFdt96yUaja3De9OROsAZlvcKE4LQB3PLssgOMCPO7Tds1KqGYUE+nHnqAQ25VrbKE4LgN2P+4+xdNchbhvenfba7lkp1cymDIklrn0ws5ekW9YoTgsAtQ3fZi9JJ6K1tntWSrUMf18f7hubRObhk3z6Y54lGbQAAKszCtmUe5Q7RyUQEuhndRyllJeY0Lcj/WPCeH55JhVVLd8ozusLQI3N8NSSDOLaBzNlSKzVcZRSXkREeHB8EodKK5i3LrfFX9/rC8DnPx0k4/AJ7hubhL+v1/9xKKVa2NDu7RmV1IE53+7hWFlli762V+/xKqpqeH55Bv27tGVC345Wx1FKeakHxidRdrqaV1dnt+jrenUBeH/DXvJLKnhgvDZ8U0pZp2dUKNcNjuH9DXs5cLS8xV7XawtASXkVr63ew2U9IxnWI8LqOEopL3f3mJ74+kiLtojw2gLw+rfZlFZUMWu8NnxTSlmvY9sg/nRJdxZty2d73vEWeU2vLAAHj5/i3fV7mTwwml6dtOGbUso13H5Zd9qHBPDE4rQWaRHhlQXg+eWZANxzRaLFSZRS6n+FBvlz5+gENua0TIsIrysAaQWlfPZTHn8YFkd0WCur4yil1C9cPySWbhEhPLk4neoaW7O+ltcVgNlL0gkN9GPG5drwTSnlevx9fbh/bCJZhSf5ZEvztojwqgKwNquYNZlFzByZQNtgf6vjKKVUg8b16cjgru14bkUmZaerm+11vKYA2GyGJxan0aVdK24a1tXqOEopdUYiwl8m9KLoxGne+j6n2V7HoQIgIuNEJENEskVkVgPrRURetq/fLiKDGttWRMJFZIWIZNm/tnPOlBr2xdaD7C4o5b6xiQT6+TbnSymlVJMN7tqOX/XtxJtrcigsrWiW12i0AIiIL/AaMB5IBq4XkeR6w8YDCfbHVGCOA9vOAlYZYxKAVfbnzaKiqoZnl2XQN7otE/t1bq6XUUopp7p/XCLVNhsvrMxslp/vyDuAIUC2MSbHGFMJLAAm1RszCXjf1NoIhIlIp0a2nQTMt38/H7i6aVM5s/fW17Z8eHBCEj4+2vJBKeUeurYP4cahcXy0+QAZh044/ec7UgCigQN1nufZlzky5mzbRhljCgDsXzs09OIiMlVEUkUktaioyIG4/1dE60CuG9xFWz4opdzOzJHxXBwfQVUznBLqyN1PGvqVuf4lamca48i2Z2WMmQvMBUhJSTmvS+OuHdyFawd3OZ9NlVLKUu1CAvjnrRc2y8925B1AHhBT53kXIN/BMWfb9rD9MBH2r9bdGVkppbyQIwVgM5AgIt1EJACYAiyqN2YRcJP9bKChQIn9sM7Ztl0E3Gz//mbgyybORSml1Dlo9BCQMaZaRO4AlgG+wDxjzC4RmWZf/wawGJgAZAPlwC1n29b+o2cDC0XkVmA/cJ1TZ6aUUuqspCU6zjlLSkqKSU1NtTqGUkq5FRHZYoxJqb/ca64EVkop9UtaAJRSyktpAVBKKS+lBUAppbyUW30ILCJFwL7z3DwCKHZiHHegc/YOOmfv0JQ5dzXGRNZf6FYFoClEJLWhT8E9mc7ZO+icvUNzzFkPASmllJfSAqCUUl7KmwrAXKsDWEDn7B10zt7B6XP2ms8AlFJK/ZI3vQNQSilVh8cVgKbcv9hdOTDnG+xz3S4i60WkvxU5namxOdcZd4GI1IjItS2Zz9kcma+IXC4iW0Vkl4isaemMzubAv+u2IvIfEdlmn/MtVuR0JhGZJyKFIrLzDOudu/8yxnjMg9qOo3uA7kAAsA1IrjdmArCE2pvVDAV+sDp3C8x5GNDO/v14b5hznXHfUNut9lqrczfz33EYsBuItT/vYHXuFpjzX4Cn7N9HAkeBAKuzN3HelwKDgJ1nWO/U/ZenvQNoyv2L3VWjczbGrDfGHLM/3UjtjXncmSN/zwAzgU9x/5sNOTLf3wGfGWP2AxhjvGHOBggVEQFaU1sAqls2pnMZY76jdh5n4tT9l6cVgKbcv9hdnet8bqX2Nwh31uicRSQamAy80YK5mosjf8c9gXYi8q2IbBGRm1osXfNwZM6vAr2ovcvgDuBOY4zzb5zrWpy6/3LknsDupCn3L3ZXDs9HREZQWwAuadZEzc+ROb8IPGCMqan9BdGtOTJfP2AwMApoBWwQkY3GmMzmDtdMHJnzWGArMBLoAawQke+NMaXNnM1KTt1/eVoBaMr9i92VQ/MRkX7A28B4Y8yRFsrWXByZcwqwwL7zjwAmiEi1MeaLFknoXI7+uy42xpQBZSLyHdAfcNcC4MicbwFmm9qD49kikgskAZtaJqIlnLr/8rRDQE25f7G7anTOIhILfAbc6Ma/EdbV6JyNMd2MMXHGmDjgE2C6m+78wbF/118Cw0XET0SCgQuBtBbO6UyOzHk/te94EJEoIBHIadGULc+p+y+PegdgmnD/Ynfl4JwfAtoDr9t/I642btxIy8E5ewxH5muMSRORpcB2wAa8bYxp8FRCd+Dg3/FjwHsisoPaQyMPGGPcukOoiHwIXA5EiEge8DDgD82z/9IrgZVSykt52iEgpZRSDtICoJRSXkoLgFJKeSktAEop5aW0ACillJfSAqCUUl5KC4BSSnkpLQBKKeWl/j/OLfK0VuIjdAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sol(x):\n",
    "    return (-x**2+x)\n",
    "#print(model(x_len))\n",
    "x_len = np.linspace(0,1,N).reshape((N,1))\n",
    "plt.plot(x_len, model(x_len)*bounday_fct(x_len))\n",
    "\n",
    "#plt.plot(x_len,sol(x_len))\n",
    "#plt.plot(x_len, np.sin(x_len))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x7fcbbc8ec4f0>]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_len = np.linspace(-1,2,1000).reshape((1000,1))\n",
    "plt.plot(x_len, model(x_len))\n",
    "plt.plot(x_len, np.exp(x_len))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}